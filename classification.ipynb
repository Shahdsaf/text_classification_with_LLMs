{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#environment setup\n",
    "#use RTL 4090 for finetuning models and A100-80GB for ICL with LLaMA 3-70B models\n",
    "!pip install transformers datasets accelerate scikit-learn evaluate wandb bitsandbytes peft --quiet\n",
    "!FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE pip install flash-attn --no-build-isolation --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data curation\n",
    "This part of the notebook is only meant for data curation of any dataset from the TweetEval datasets: https://github.com/cardiffnlp/tweeteval/tree/main/datasets\n",
    "please skip this part if you want to read directly the parquet files under \"data\" folder for the emotion classification task from TweetEval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open(\"data/mapping.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "    lines = [line.split(\"\\t\") for line in lines]\n",
    "\n",
    "label2id = {line[1]: int(line[0]) for line in lines}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "print(label2id)\n",
    "print(id2label)\n",
    "\n",
    "\n",
    "def convert_txt_to_parquet(mode=\"train\"):\n",
    "    with open(f\"data/{mode}_text.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    df = pd.DataFrame(lines, columns=[\"text\"])\n",
    "\n",
    "    with open(f\"data/{mode}_labels.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "        lines = [int(line) for line in lines]\n",
    "\n",
    "    df[\"label\"] = lines\n",
    "    df[\"label_text\"] = df[\"label\"].apply(lambda x: id2label[x])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "dfs = {}\n",
    "for mode in [\"train\", \"val\", \"test\"]:\n",
    "    dfs[mode] = convert_txt_to_parquet(mode=mode)\n",
    "\n",
    "\n",
    "def clean(df):\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates(subset=[\"text\"])\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train = clean(dfs[\"train\"])\n",
    "val = clean(dfs[\"val\"])\n",
    "test = clean(dfs[\"test\"])\n",
    "\n",
    "# convert label column from str to int\n",
    "train[\"label\"] = train[\"label\"].astype(int)\n",
    "val[\"label\"] = val[\"label\"].astype(int)\n",
    "test[\"label\"] = test[\"label\"].astype(int)\n",
    "\n",
    "\n",
    "train.to_parquet(\"data/train.parquet\")\n",
    "val.to_parquet(\"data/val.parquet\")\n",
    "test.to_parquet(\"data/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and tokenization for finetuning or ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and val from parquet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open(\"data/mapping.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "    lines = [line.split(\"\\t\") for line in lines]\n",
    "\n",
    "label2id = {line[1]: int(line[0]) for line in lines}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "print(label2id)\n",
    "print(id2label)\n",
    "\n",
    "train = pd.read_parquet(\"data/train.parquet\")\n",
    "val = pd.read_parquet(\"data/val.parquet\")\n",
    "test = pd.read_parquet(\"data/test.parquet\")\n",
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading to HF datasets\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train, split=\"train\")\n",
    "val_dataset = Dataset.from_pandas(val, split=\"val\")\n",
    "test_dataset = Dataset.from_pandas(test, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used only for finetuning with weighted crossentropy\n",
    "\n",
    "import torch\n",
    "\n",
    "label_id_df = train[\"label_text\"].map(label2id)\n",
    "\n",
    "class_weights = (1 / label_id_df.value_counts(normalize=True).sort_index()).tolist()\n",
    "class_weights = torch.tensor(class_weights)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model and training config and wandb config\n",
    "\n",
    "%env WANDB_WATCH=all\n",
    "%env WANDB_SILENT=true\n",
    "#%env WANDB_LOG_MODEL=end  #used when saving the model checkpoint to wandb\n",
    "%env WANDB_LOG_MODEL=false\n",
    "%env WANDB_PROJECT=text_classification_with_LLMs\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"vinai/bertweet-base\", #\"bert-base-uncased\", #\"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"version\": \"2\",    #run number\n",
    "    \"batch_size\": 64,\n",
    "    \"train_epochs\": 100,\n",
    "    \"num_workers\": 8,\n",
    "    \"lr\": 0.00003,\n",
    "    #\"dropout\": 0.3\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer and preprocessing\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "if \"llama\" in config[\"model_name\"]:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        token=\"HF_token\",\n",
    "        add_prefix_space=True,\n",
    "    )\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"llama tokenizer\")\n",
    "\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        token=\"HF_token\",\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data collator with dynamic padding\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics: f1, precision, recall (weighted mode) and f1 per label\n",
    "\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.combine([\"f1\", \"precision\", \"recall\"])\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    metrics = metric.compute(\n",
    "        predictions=predictions, references=labels, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    # F1 Score per label\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    f1_per_label = f1_metric.compute(\n",
    "        predictions=predictions, references=labels, average=None\n",
    "    )\n",
    "    f1_per_label_dict = {id2label[idx]: v for idx, v in enumerate(f1_per_label[\"f1\"])}\n",
    "\n",
    "    return {\n",
    "        \"f1\": metrics[\"f1\"],\n",
    "        \"precision\": metrics[\"precision\"],\n",
    "        \"recall\": metrics[\"recall\"],\n",
    "        \"f1_per_label\": f1_per_label_dict,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "if \"llama\" in config[\"model_name\"]:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        num_labels=6,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        token=\"HF_token\",\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    print(\"llama model\")\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        num_labels=4,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        token=\"HF_token\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the lora config and use this cell only if LLAMA 3-8B is used\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell only if dropout to be changed for Bert-like models\n",
    "# import torch.nn as nn\n",
    "\n",
    "# model.dropout = nn.Dropout(config[\"dropout\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params\n",
    "\n",
    "\n",
    "count_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training arguments\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=4)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{config['model_name']}-v-{config['version']}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=config[\"lr\"],\n",
    "    per_device_train_batch_size=config[\"batch_size\"],\n",
    "    per_device_eval_batch_size=config[\"batch_size\"],\n",
    "    num_train_epochs=config[\"train_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    push_to_hub=False,\n",
    "    # fp16=True,\n",
    "    # fp16_full_eval=True,\n",
    "    bf16_full_eval=True,\n",
    "    bf16=True,\n",
    "    save_total_limit=1,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"wandb\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_ratio=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    run_name=f\"{config['model_name']}-v-{config['version']}\",\n",
    "    dataloader_num_workers=config[\"num_workers\"],\n",
    "    # gradient_accumulation_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the trainer that performs the training with weighted crossentropy\n",
    "# use only for finetuning with weighted crossentropy\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(\n",
    "                self.args.device\n",
    "            )\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "# trainer = CustomTrainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=tokenized_train,\n",
    "#     eval_dataset=tokenized_val,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     callbacks=[early_stopping],\n",
    "#     data_collator=data_collator,\n",
    "#     class_weights=class_weights,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model and finish the wandb run\n",
    "import wandb\n",
    "\n",
    "wandb.config.update(config)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero/Few Shot In-Context Learning (ICL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the pipeline with LLaMA 3-70B\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_storage=torch.float16,\n",
    ")\n",
    "\n",
    "# model_id = \"meta-llama/Meta-Llama-3-70B\"\n",
    "model_id = (\n",
    "    \"meta-llama/Meta-Llama-3-70B-Instruct\"  # \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=\"HF_token\",  # add_prefix_space=True\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\n",
    "        \"quantization_config\": bnb_config,\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "        \"attn_implementation\": \"sdpa\",\n",
    "    },\n",
    "    device_map=\"auto\",\n",
    "    token=\"HF_token\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define few-shot examples\n",
    "# use in case of few shot ICL with random selection of few-shot examples\n",
    "nr_of_shots = 2\n",
    "sampled_tweets = (\n",
    "    train.groupby(\"label\")\n",
    "    .apply(lambda x: x.sample(n=nr_of_shots))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create a defaultdict with list as the default factory\n",
    "label_tweet_dict = defaultdict(list)\n",
    "\n",
    "# Iterate over the DataFrame rows and append questions to the appropriate label list\n",
    "for _, row in sampled_tweets.iterrows():\n",
    "    label_tweet_dict[row[\"label\"]].append(row[\"text\"])\n",
    "\n",
    "# Convert defaultdict to a regular dictionary if desired\n",
    "label_tweet_dict = dict(label_tweet_dict)\n",
    "\n",
    "one_shot_examples = label_tweet_dict\n",
    "\n",
    "one_shot_examples_string = \"\"\n",
    "for label, tweets in one_shot_examples.items():\n",
    "    for tweet in tweets:\n",
    "        one_shot_examples_string += f\"Text: {tweet}. Answer: {label}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that finds the top N best matching tweets per label for a given query tweet and\n",
    "# format these as a string to be used in the instruct prompt\n",
    "# use in case of few shot ICL with a heuristic selection of few-shot examples\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "val_embeddings = np.load(\"val_tweet_bert_embeddings.npy\")\n",
    "train_embeddings = np.load(\"train_tweet_bert_embeddings.npy\")\n",
    "\n",
    "\n",
    "def find_top_matches_per_label(val_embeddings, train_embeddings, train_df, top_n=2):\n",
    "    \"\"\"\n",
    "    Finds the top N best matching tweets per label in the training set for each query in the validation set.\n",
    "\n",
    "    Parameters:\n",
    "    - val_embeddings (numpy.ndarray): The embeddings of the validation set tweets.\n",
    "    - train_embeddings (numpy.ndarray): The embeddings of the training set tweets.\n",
    "    - train_df (pandas.DataFrame): The training DataFrame containing \"tweet\" and \"label\" columns.\n",
    "    - top_n (int): The number of top matches per label to return for each validation query.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where each key is a validation query index and the value is a dictionary\n",
    "            with labels as keys and the top N matching tweets as values.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over each validation query\n",
    "    for i, query_embedding in enumerate(val_embeddings):\n",
    "        # Compute similarities between the query and all training embeddings\n",
    "        similarities = np.dot(train_embeddings, query_embedding)\n",
    "\n",
    "        # Create a DataFrame to store similarity scores with corresponding tweets and labels\n",
    "        similarity_df = pd.DataFrame(\n",
    "            {\n",
    "                \"text\": train_df[\"text\"],\n",
    "                \"label\": train_df[\"label\"],\n",
    "                \"similarity\": similarities,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Group by label and get the top N matches per label\n",
    "        top_matches_per_label = (\n",
    "            similarity_df.groupby(\"label\")\n",
    "            .apply(lambda x: x.nlargest(top_n, \"similarity\"))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Store the results for this query\n",
    "        results[i] = {}\n",
    "        for label in top_matches_per_label[\"label\"].unique():\n",
    "            best_matches = top_matches_per_label[\n",
    "                top_matches_per_label[\"label\"] == label\n",
    "            ][\"text\"].tolist()\n",
    "            results[i][label] = best_matches\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have your dataframes `val` and `train` and the corresponding embeddings `val_embeddings` and `train_embeddings`\n",
    "top_n = 1  # Number of top matches per label\n",
    "results = find_top_matches_per_label(val_embeddings, train_embeddings, train, top_n)\n",
    "\n",
    "\n",
    "one_shot_examples = []\n",
    "\n",
    "for i, one_shot_example in results.items():\n",
    "    one_shot_examples_string = \"\"\n",
    "    for label, tweets in one_shot_example.items():\n",
    "        for tweet in tweets:\n",
    "            one_shot_examples_string += f\"Text: {tweet}. Answer: {label}\\n\\n\"\n",
    "\n",
    "    one_shot_examples.append(one_shot_examples_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform Zero-Shot or Few-Shot Classification\n",
    "# for zero shot remove from the prompt:\n",
    "#   {context}\n",
    "#   {task_info}\n",
    "#   {few_shots}\n",
    "# for few shot only use this cell if you rely on random selection of few-shot examples\n",
    "\n",
    "target = \"EMOTION TYPE\"\n",
    "target_with_brackets = f\"{{{target}}}\"\n",
    "labels = list(label2id.keys())\n",
    "label_token_lists = {}\n",
    "for l in list(label2id.keys()):\n",
    "    label_token_lists[l] = tokenizer.encode(l, add_special_tokens=False)\n",
    "\n",
    "\n",
    "def match_label(generated_string_tokens, ground_truth, label_token_lists):\n",
    "\n",
    "    def is_subsequence(subseq, seq):\n",
    "        \"\"\"\n",
    "        Check if subseq is a contiguous subsequence of seq.\n",
    "\n",
    "        Args:\n",
    "            subseq (list): The subsequence to check.\n",
    "            seq (list): The sequence to check against.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if subseq is a subsequence of seq, else False.\n",
    "        \"\"\"\n",
    "        sub_len = len(subseq)\n",
    "        for i in range(len(seq) - sub_len + 1):\n",
    "            if seq[i : i + sub_len] == subseq:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    matching_labels = []\n",
    "\n",
    "    # Check each label token list to see if it is a subsequence of the generated string tokens\n",
    "    for label, label_tokens in label_token_lists.items():\n",
    "        if is_subsequence(label_tokens, generated_string_tokens):\n",
    "            matching_labels.append(label)\n",
    "\n",
    "    # Return the label if only one matches, otherwise return None\n",
    "    if len(matching_labels) == 1 and matching_labels[0] == ground_truth:\n",
    "        return True  # matching_labels[0]\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "context = \"This tweet is posted on the social media platform Twitter and you should classify it based on the emotion this tweet expresses.\"\n",
    "\n",
    "task_info = \"\"\"This tweet is posted on Twitter and the text expresses one of the following emotions:\n",
    "- `anger`: Feelings of frustration or hostility, often expressed through irritation or outrage.\n",
    "- `joy`: Positive feelings of happiness or pleasure, shown through excitement or delight.\n",
    "- `optimism`: Hopeful and positive outlook, indicating encouragement or belief in good outcomes.\n",
    "- `sadness`: Feelings of sorrow or disappointment, expressed through a sense of loss or melancholy.\n",
    "\"\"\"\n",
    "\n",
    "few_shots = f\"\"\"Here are some examples of tweets that are classified based on their emotion type:\n",
    "\n",
    "{one_shot_examples_string}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def classify(text):\n",
    "    instructions = f\"\"\"  \n",
    "    {context} \n",
    "    {task_info}\n",
    "    {few_shots}\n",
    "    \n",
    "    Return the {target} {labels} with one word/label response without any additional text. Answer: {target_with_brackets}.\"\"\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "    Text: {text}. Answer: \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "    ]\n",
    "    output = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=3,  # 64, 128, 256, 4096   #max_length\n",
    "        pad_token_id=pipeline.tokenizer.pad_token_id,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "        # top_k=1,\n",
    "        temperature=0.0,  # 0.1, 0.3, 0.6, 0.8\n",
    "        # top_p=0.9,  # 0.8, 0.9, 0.95\n",
    "        return_full_text=False,\n",
    "    )[0][\"generated_text\"].lstrip()\n",
    "    return output\n",
    "\n",
    "\n",
    "def final_output(text, ground_truth):\n",
    "    output = classify(text)\n",
    "    generated_string_tokens = tokenizer.encode(output, add_special_tokens=False)\n",
    "    if match_label(generated_string_tokens, ground_truth, label_token_lists):\n",
    "        return label2id[ground_truth]\n",
    "    else:\n",
    "        # choose random label from remaining ones excluding ground truth\n",
    "        remaining_labels = [l for l in labels if l != ground_truth]\n",
    "        random_label = np.random.choice(remaining_labels)\n",
    "        return label2id[random_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for few shot with a heuristic selection of few-shot examples\n",
    "\n",
    "\n",
    "target = \"EMOTION TYPE\"\n",
    "target_with_brackets = f\"{{{target}}}\"\n",
    "labels = list(label2id.keys())\n",
    "label_token_lists = {}\n",
    "for l in list(label2id.keys()):\n",
    "    label_token_lists[l] = tokenizer.encode(l, add_special_tokens=False)\n",
    "\n",
    "\n",
    "def match_label(generated_string_tokens, ground_truth, label_token_lists):\n",
    "\n",
    "    def is_subsequence(subseq, seq):\n",
    "        \"\"\"\n",
    "        Check if subseq is a contiguous subsequence of seq.\n",
    "\n",
    "        Args:\n",
    "            subseq (list): The subsequence to check.\n",
    "            seq (list): The sequence to check against.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if subseq is a subsequence of seq, else False.\n",
    "        \"\"\"\n",
    "        sub_len = len(subseq)\n",
    "        for i in range(len(seq) - sub_len + 1):\n",
    "            if seq[i : i + sub_len] == subseq:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    matching_labels = []\n",
    "\n",
    "    # Check each label token list to see if it is a subsequence of the generated string tokens\n",
    "    for label, label_tokens in label_token_lists.items():\n",
    "        if is_subsequence(label_tokens, generated_string_tokens):\n",
    "            matching_labels.append(label)\n",
    "\n",
    "    # Return the label if only one matches, otherwise return None\n",
    "    if len(matching_labels) == 1 and matching_labels[0] == ground_truth:\n",
    "        return True  # matching_labels[0]\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "context = \"This tweet is posted on the social media platform Twitter and you should classify it based on the emotion this tweet expresses.\"\n",
    "\n",
    "task_info = \"\"\"This tweet is posted on Twitter and the text expresses one of the following emotions:\n",
    "- `anger`: Feelings of frustration or hostility, often expressed through irritation or outrage.\n",
    "- `joy`: Positive feelings of happiness or pleasure, shown through excitement or delight.\n",
    "- `optimism`: Hopeful and positive outlook, indicating encouragement or belief in good outcomes.\n",
    "- `sadness`: Feelings of sorrow or disappointment, expressed through a sense of loss or melancholy.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def classify(text, one_shot_examples_string):\n",
    "\n",
    "    few_shots = f\"\"\"Here are some examples of questions that are classified based on their type:\n",
    "\n",
    "    {one_shot_examples_string}\n",
    "    \"\"\"\n",
    "    instructions = f\"\"\"\n",
    "    {context}\n",
    "    {task_info}\n",
    "    {few_shots}\n",
    "    \n",
    "    Return the {target} {labels} with one word/label response without any additional text. Answer: {target_with_brackets}.\"\"\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "    Text: {text}. Answer: \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "    ]\n",
    "    output = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=3,  # 64, 128, 256, 4096   #max_length\n",
    "        pad_token_id=pipeline.tokenizer.pad_token_id,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "        # top_k=1,\n",
    "        temperature=0.0,  # 0.1, 0.3, 0.6, 0.8\n",
    "        # top_p=0.9,  # 0.8, 0.9, 0.95\n",
    "        return_full_text=False,\n",
    "    )[0][\"generated_text\"].lstrip()\n",
    "    return output\n",
    "\n",
    "\n",
    "def final_output(tweet, ground_truth, one_shot_examples_string):\n",
    "    output = classify(tweet, one_shot_examples_string)\n",
    "    generated_string_tokens = tokenizer.encode(output, add_special_tokens=False)\n",
    "    if match_label(generated_string_tokens, ground_truth, label_token_lists):\n",
    "        return label2id[ground_truth]\n",
    "    else:\n",
    "        # choose random label from remaining ones excluding ground truth\n",
    "        remaining_labels = [l for l in labels if l != ground_truth]\n",
    "        random_label = np.random.choice(remaining_labels)\n",
    "        return label2id[random_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given val dataset, generate predictions\n",
    "\n",
    "tweets = val[\"text\"].tolist()\n",
    "ground_truths = val[\"label_text\"].tolist()\n",
    "\n",
    "preds = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "for i in tqdm(range(len(tweets))):\n",
    "    # one_shot_examples_string = one_shot_examples[i]\n",
    "    tweet = tweets[i]\n",
    "    ground_truth = ground_truths[i]\n",
    "    # pred = final_output(question, ground_truth, one_shot_examples_string)\n",
    "    pred = final_output(tweet, ground_truth)\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ground truths to ids\n",
    "ground_truths_ids = [label2id[l] for l in ground_truths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given preds and ground truth, calculate f1 score weighted\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.combine([\"f1\", \"precision\", \"recall\"])\n",
    "metrics = metric.compute(\n",
    "    predictions=preds, references=ground_truths_ids, average=\"weighted\"\n",
    ")\n",
    "# F1 Score per label\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "f1_per_label = f1_metric.compute(\n",
    "    predictions=preds, references=ground_truths_ids, average=None\n",
    ")\n",
    "f1_per_label_dict = {id2label[idx]: v for idx, v in enumerate(f1_per_label[\"f1\"])}\n",
    "\n",
    "metrics.update(f1_per_label_dict)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Bert embeddings for train or val data and save them to disk\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Check if CUDA is available and use it if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embeddings(text):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "        )\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        # Get the embeddings of [CLS] token, which is the first token\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Initialize an empty list to store the embeddings\n",
    "embeddings_list = []\n",
    "\n",
    "# Iterate over the questions in the dataset with a progress bar\n",
    "for text in tqdm(val_dataset[\"text\"], desc=\"Generating embeddings\"):\n",
    "    embedding = get_embeddings(text)\n",
    "    embedding = embedding / norm(embedding, axis=1, keepdims=True)\n",
    "    embeddings_list.append(embedding)\n",
    "\n",
    "# Convert the list of embeddings to a NumPy array\n",
    "embeddings = np.vstack(embeddings_list)\n",
    "\n",
    "# Save embeddings to a numpy file\n",
    "np.save(\"val_tweet_bert_embeddings.npy\", embeddings)\n",
    "\n",
    "print(\"Embeddings saved to 'val_tweet_bert_embeddings.npy'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
