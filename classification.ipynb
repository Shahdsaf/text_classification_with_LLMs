{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets accelerate scikit-learn evaluate wandb bitsandbytes peft --quiet\n",
    "!FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE pip install flash-attn --no-build-isolation --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation and splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Troubleshooting</td>\n",
       "      <td>Build Error\\nWhen I try to import an image sta...</td>\n",
       "      <td>Next.js handles static image imports slightly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>How can I specify my own character to separate...</td>\n",
       "      <td>I'm sorry, but based on the provided knowledge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>When can I buy Tari?</td>\n",
       "      <td>As of the latest updates, the Tari main net ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Off-Topic</td>\n",
       "      <td>Invest</td>\n",
       "      <td>I'm sorry, but your question is a bit vague. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>I there a way to define a global variable that...</td>\n",
       "      <td>Yes, in Mage, you can define global variables ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          category                                           question  \\\n",
       "0  Troubleshooting  Build Error\\nWhen I try to import an image sta...   \n",
       "1        Discovery  How can I specify my own character to separate...   \n",
       "2        Discovery                               When can I buy Tari?   \n",
       "3        Off-Topic                                             Invest   \n",
       "4        Discovery  I there a way to define a global variable that...   \n",
       "\n",
       "                                              answer  \n",
       "0  Next.js handles static image imports slightly ...  \n",
       "1  I'm sorry, but based on the provided knowledge...  \n",
       "2  As of the latest updates, the Tari main net ha...  \n",
       "3  I'm sorry, but your question is a bit vague. I...  \n",
       "4  Yes, in Mage, you can define global variables ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "with open(\"data.json\", \"r\") as f:\n",
    "    data = json.load(f)[\"data\"]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Discovery          734\n",
       "Troubleshooting    199\n",
       "Off-Topic           83\n",
       "Code                59\n",
       "Advice              53\n",
       "Comparison          50\n",
       "Off-topic            7\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Discovery          734\n",
       "Troubleshooting    199\n",
       "Off-Topic           90\n",
       "Code                59\n",
       "Advice              53\n",
       "Comparison          50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are 2 identical categories: \"Off-topic\" and \"Off-Topic\"\n",
    "# rename to \"Off-Topic\"\n",
    "\n",
    "df[\"category\"] = df[\"category\"].str.replace(\"Off-topic\", \"Off-Topic\")\n",
    "\n",
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Code</td>\n",
       "      <td>can you please convert this sample code in jav...</td>\n",
       "      <td>Sure, I can help you convert the JavaScript co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>how can i get mask all coordinate</td>\n",
       "      <td>Hmm, I don't know enough to give you a confide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>if a vulnerable package is already cached insi...</td>\n",
       "      <td>No, if a vulnerable package is already cached ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>how to get SMS for login</td>\n",
       "      <td>FusionAuth supports SMS-based multi-factor aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>Has anyone mentioned that the changelog or wee...</td>\n",
       "      <td>I'm sorry, but the provided knowledge sources ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                           question  \\\n",
       "72        Code  can you please convert this sample code in jav...   \n",
       "351  Discovery                  how can i get mask all coordinate   \n",
       "212  Discovery  if a vulnerable package is already cached insi...   \n",
       "17   Discovery                           how to get SMS for login   \n",
       "7    Discovery  Has anyone mentioned that the changelog or wee...   \n",
       "\n",
       "                                                answer  \n",
       "72   Sure, I can help you convert the JavaScript co...  \n",
       "351  Hmm, I don't know enough to give you a confide...  \n",
       "212  No, if a vulnerable package is already cached ...  \n",
       "17   FusionAuth supports SMS-based multi-factor aut...  \n",
       "7    I'm sorry, but the provided knowledge sources ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given labels are not equally distributed, split by category with stratify\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train, test = train_test_split(\n",
    "    df, test_size=0.15, stratify=df[\"category\"], random_state=42, shuffle=True\n",
    ")\n",
    "train, val = train_test_split(\n",
    "    train, test_size=0.15, stratify=train[\"category\"], random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index and save to parquet\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "# for each row, add to question column f\"Question: {row['question']}\" and answer column f\"Answer: {row['answer']}\" and create a new column combined question and answer in one column\n",
    "\n",
    "train[\"question\"] = train[\"question\"].apply(lambda x: f\"Question: {x}\")\n",
    "train[\"answer\"] = train[\"answer\"].apply(lambda x: f\"Answer: {x}\")\n",
    "\n",
    "val[\"question\"] = val[\"question\"].apply(lambda x: f\"Question: {x}\")\n",
    "val[\"answer\"] = val[\"answer\"].apply(lambda x: f\"Answer: {x}\")\n",
    "\n",
    "test[\"question\"] = test[\"question\"].apply(lambda x: f\"Question: {x}\")\n",
    "test[\"answer\"] = test[\"answer\"].apply(lambda x: f\"Answer: {x}\")\n",
    "\n",
    "train[\"combined\"] = train[\"question\"] + \"\\n\" + train[\"answer\"]\n",
    "val[\"combined\"] = val[\"question\"] + \"\\n\" + val[\"answer\"]\n",
    "test[\"combined\"] = test[\"question\"] + \"\\n\" + test[\"answer\"]\n",
    "\n",
    "\n",
    "train.to_parquet(\"train.parquet\")\n",
    "val.to_parquet(\"val.parquet\")\n",
    "test.to_parquet(\"test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Discovery          530\n",
       "Troubleshooting    143\n",
       "Off-Topic           65\n",
       "Code                42\n",
       "Advice              38\n",
       "Comparison          37\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read train and val from parquet\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train = pd.read_parquet(\"train.parquet\")\n",
    "val = pd.read_parquet(\"val.parquet\")\n",
    "test = pd.read_parquet(\"test.parquet\")\n",
    "\n",
    "# change column name from category to label\n",
    "train = train.rename(columns={\"category\": \"label\"})\n",
    "val = val.rename(columns={\"category\": \"label\"})\n",
    "test = test.rename(columns={\"category\": \"label\"})\n",
    "\n",
    "train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label       0\n",
      "question    0\n",
      "answer      0\n",
      "combined    0\n",
      "dtype: int64\n",
      "label       0\n",
      "question    0\n",
      "answer      0\n",
      "combined    0\n",
      "dtype: int64\n",
      "label       0\n",
      "question    0\n",
      "answer      0\n",
      "combined    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# get nan values for train and val and test\n",
    "\n",
    "print(train.isna().sum())\n",
    "print(val.isna().sum())\n",
    "print(test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>Question: OAPåœ¨Receiveræ¨¡å¼ä¸‹, å¿…é¡»è¦é…ç½®æˆé›†ç¾¤æ¨¡å¼å—?</td>\n",
       "      <td>Answer: æ˜¯çš„ï¼ŒOAPåœ¨Receiveræ¨¡å¼ä¸‹åº”è¯¥é…ç½®æˆé›†ç¾¤æ¨¡å¼ã€‚åœ¨é›†ç¾¤æ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰çš„...</td>\n",
       "      <td>Question: OAPåœ¨Receiveræ¨¡å¼ä¸‹, å¿…é¡»è¦é…ç½®æˆé›†ç¾¤æ¨¡å¼å—?\\nAnswe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Troubleshooting</td>\n",
       "      <td>Question: Build Error\\nWhen I try to import an...</td>\n",
       "      <td>Answer: Next.js handles static image imports s...</td>\n",
       "      <td>Question: Build Error\\nWhen I try to import an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Off-Topic</td>\n",
       "      <td>Question: Hello there</td>\n",
       "      <td>Answer: Hello! How can I assist you with Typef...</td>\n",
       "      <td>Question: Hello there\\nAnswer: Hello! How can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Off-Topic</td>\n",
       "      <td>Question: hi</td>\n",
       "      <td>Answer: Hello! How can I assist you with kapa....</td>\n",
       "      <td>Question: hi\\nAnswer: Hello! How can I assist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>Question: creating a new project in amplitude</td>\n",
       "      <td>Answer: Creating a new project in Amplitude in...</td>\n",
       "      <td>Question: creating a new project in amplitude ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label                                           question  \\\n",
       "0        Discovery            Question: OAPåœ¨Receiveræ¨¡å¼ä¸‹, å¿…é¡»è¦é…ç½®æˆé›†ç¾¤æ¨¡å¼å—?   \n",
       "1  Troubleshooting  Question: Build Error\\nWhen I try to import an...   \n",
       "2        Off-Topic                              Question: Hello there   \n",
       "3        Off-Topic                                       Question: hi   \n",
       "4        Discovery     Question: creating a new project in amplitude    \n",
       "\n",
       "                                              answer  \\\n",
       "0  Answer: æ˜¯çš„ï¼ŒOAPåœ¨Receiveræ¨¡å¼ä¸‹åº”è¯¥é…ç½®æˆé›†ç¾¤æ¨¡å¼ã€‚åœ¨é›†ç¾¤æ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰çš„...   \n",
       "1  Answer: Next.js handles static image imports s...   \n",
       "2  Answer: Hello! How can I assist you with Typef...   \n",
       "3  Answer: Hello! How can I assist you with kapa....   \n",
       "4  Answer: Creating a new project in Amplitude in...   \n",
       "\n",
       "                                            combined  \n",
       "0  Question: OAPåœ¨Receiveræ¨¡å¼ä¸‹, å¿…é¡»è¦é…ç½®æˆé›†ç¾¤æ¨¡å¼å—?\\nAnswe...  \n",
       "1  Question: Build Error\\nWhen I try to import an...  \n",
       "2  Question: Hello there\\nAnswer: Hello! How can ...  \n",
       "3  Question: hi\\nAnswer: Hello! How can I assist ...  \n",
       "4  Question: creating a new project in amplitude ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfdc4293beb47039f76e59490e56539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/855 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7457c3d313c1496a95b19c7d69ceed8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a045d9e4954aa7814536037550c629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train, split=\"train\").class_encode_column(\"label\")\n",
    "class_label_feature = train_dataset.features[\"label\"]\n",
    "val_dataset = Dataset.from_pandas(val, split=\"val\").cast_column(\n",
    "    \"label\", class_label_feature\n",
    ")\n",
    "test_dataset = Dataset.from_pandas(test, split=\"test\").cast_column(\n",
    "    \"label\", class_label_feature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Advice',\n",
       " 1: 'Code',\n",
       " 2: 'Comparison',\n",
       " 3: 'Discovery',\n",
       " 4: 'Off-Topic',\n",
       " 5: 'Troubleshooting'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = train_dataset.features[\"label\"]._str2int\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# label2id = train_dataset.features['label'].names\n",
    "# id2label = {i: label for i, label in enumerate(label2id)}\n",
    "\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2595, 0.2348, 0.2665, 0.0186, 0.1517, 0.0690])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "label_id_df = train[\"label\"].map(label2id)\n",
    "\n",
    "class_weights = (1 / label_id_df.value_counts(normalize=True).sort_index()).tolist()\n",
    "class_weights = torch.tensor(class_weights)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_WATCH=all\n",
      "env: WANDB_SILENT=true\n",
      "env: WANDB_LOG_MODEL=false\n",
      "env: WANDB_PROJECT=kapa_question_type_classification\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_WATCH=all\n",
    "%env WANDB_SILENT=true\n",
    "#%env WANDB_LOG_MODEL=end\n",
    "%env WANDB_LOG_MODEL=false\n",
    "%env WANDB_PROJECT=kapa_question_type_classification\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"input_type\": \"question\",\n",
    "    \"version\": \"1-9\",\n",
    "    \"batch_size\": 16,\n",
    "    \"train_epochs\": 100,\n",
    "    \"num_workers\": 8,\n",
    "    \"lr\": 1e-4,\n",
    "    #\"dropout\": 0.3\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama tokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4d4736ac184b1fb6cbe335b4edc45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/855 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e202a4724344ec4b0af2cddde074323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb82c6aa26354e8991059e259878ba59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "if \"llama\" in config[\"model_name\"]:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        token=\"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\",\n",
    "        add_prefix_space=True,\n",
    "    )\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"llama tokenizer\")\n",
    "\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        token=\"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\",\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[config[\"input_type\"]], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Question: OAPåœ¨Receiveræ¨¡å¼ä¸‹, å¿…é¡»è¦é…ç½®æˆé›†ç¾¤æ¨¡å¼å—?\\nAnswer: æ˜¯çš„ï¼ŒOAPåœ¨Receiveræ¨¡å¼ä¸‹åº”è¯¥é…ç½®æˆé›†ç¾¤æ¨¡å¼ã€‚åœ¨é›†ç¾¤æ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰çš„OAPèŠ‚ç‚¹é»˜è®¤éƒ½åœ¨Mixedæ¨¡å¼ä¸‹è¿è¡Œã€‚ç„¶è€Œï¼Œæœ‰æ—¶ç”¨æˆ·å¯èƒ½å¸Œæœ›ä»¥æ˜ç¡®çš„è§’è‰²éƒ¨ç½²é›†ç¾¤èŠ‚ç‚¹ï¼Œä»–ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªåŠŸèƒ½ã€‚\\n\\nåœ¨Receiveræ¨¡å¼ä¸‹ï¼ŒOAPè´Ÿè´£ï¼š\\n1. æ¥æ”¶agentçš„è·Ÿè¸ªæˆ–æŒ‡æ ‡ã€‚\\n2. L1èšåˆ\\n3. å†…éƒ¨é€šä¿¡ï¼ˆå‘é€ï¼‰\\n\\nè¿™äº›è§’è‰²æ˜¯ä¸ºäº†æ»¡è¶³å®‰å…¨å’Œç½‘ç»œç­–ç•¥ä¸Šçš„å¤æ‚éƒ¨ç½²éœ€æ±‚è€Œè®¾è®¡çš„ã€‚å¦‚æœä½ æ­£åœ¨ä½¿ç”¨æˆ‘ä»¬çš„åŸç”Ÿ[Kubernetesåè°ƒå™¨](https://skywalking.apache.org/docs/main/latest/en/setup/backend-cluster#kubernetes)ï¼Œå¹¶ä¸”ä½ åšæŒè¦ä»¥æ˜ç¡®çš„è§’è‰²å®‰è£…OAPèŠ‚ç‚¹ã€‚é‚£ä¹ˆæ¯ä¸ªè§’è‰²åº”è¯¥æœ‰ä¸¤ä¸ªéƒ¨ç½²ï¼Œä¸€ä¸ªç”¨äºæ¥æ”¶å™¨OAPï¼Œå¦ä¸€ä¸ªç”¨äºèšåˆå™¨OAPï¼Œä»¥åˆ†éš”ä¸åŒçš„ç³»ç»Ÿç¯å¢ƒè®¾ç½®ã€‚ç„¶åï¼Œåº”è¯¥ä¸º`Aggregator`è§’è‰²é€‰æ‹©è§„åˆ™è®¾ç½®`labelSelector`ï¼Œä»¥æ ¹æ®ä½ çš„éœ€æ±‚é€‰æ‹©æ­£ç¡®çš„OAPéƒ¨ç½²ã€‚\\n\\nå‚è€ƒèµ„æ–™ï¼š[SkyWalking Advanced Deployment](https://skywalking.apache.org/docs/main/latest/en/setup/backend/advanced-deployment#advanced-deployment)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_test[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: OAPåœ¨Receiveræ¨¡å¼ä¸‹, å¿…é¡»è¦é…ç½®æˆé›†ç¾¤æ¨¡å¼å—?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test[0][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.combine([\"f1\", \"precision\", \"recall\"])\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    metrics = metric.compute(\n",
    "        predictions=predictions, references=labels, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    # F1 Score per label\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    f1_per_label = f1_metric.compute(\n",
    "        predictions=predictions, references=labels, average=None\n",
    "    )\n",
    "    f1_per_label_dict = {id2label[idx]: v for idx, v in enumerate(f1_per_label[\"f1\"])}\n",
    "\n",
    "    return {\n",
    "        \"f1\": metrics[\"f1\"],\n",
    "        \"precision\": metrics[\"precision\"],\n",
    "        \"recall\": metrics[\"recall\"],\n",
    "        \"f1_per_label\": f1_per_label_dict,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faeac4af927540ce89ad76d4686d8649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama model\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "if \"llama\" in config[\"model_name\"]:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        num_labels=6,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        token=\"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\",\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    print(\"llama model\")\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        num_labels=6,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        token=\"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\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# model.dropout = nn.Dropout(config[\"dropout\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13656064"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params\n",
    "\n",
    "\n",
    "count_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=4)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{config['model_name']}-v-{config['version']}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=config[\"lr\"],\n",
    "    per_device_train_batch_size=config[\"batch_size\"],\n",
    "    per_device_eval_batch_size=config[\"batch_size\"],\n",
    "    num_train_epochs=config[\"train_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    push_to_hub=False,\n",
    "    # fp16=True,\n",
    "    # fp16_full_eval=True,\n",
    "    bf16_full_eval=True,\n",
    "    bf16=True,\n",
    "    save_total_limit=1,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"wandb\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    run_name=f\"{config['model_name']}-v-{config['version']}\",\n",
    "    dataloader_num_workers=config[\"num_workers\"],\n",
    "    # gradient_accumulation_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(\n",
    "                self.args.device\n",
    "            )\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"label\").long()\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "# trainer = CustomTrainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=tokenized_train,\n",
    "#     eval_dataset=tokenized_val,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     callbacks=[early_stopping],\n",
    "#     data_collator=data_collator,\n",
    "#     class_weights=class_weights,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1605' max='10700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1605/10700 1:12:18 < 6:50:18, 0.37 it/s, Epoch 15/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Per Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.773900</td>\n",
       "      <td>1.114577</td>\n",
       "      <td>0.669231</td>\n",
       "      <td>0.717671</td>\n",
       "      <td>0.690789</td>\n",
       "      <td>{'Advice': 0.11764705882352941, 'Code': 0.2222222222222222, 'Comparison': 0.5, 'Discovery': 0.8076923076923077, 'Off-Topic': 0.47058823529411764, 'Troubleshooting': 0.5777777777777777}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.756500</td>\n",
       "      <td>0.831966</td>\n",
       "      <td>0.792158</td>\n",
       "      <td>0.783520</td>\n",
       "      <td>0.822368</td>\n",
       "      <td>{'Advice': 0.0, 'Code': 0.3076923076923077, 'Comparison': 0.6666666666666666, 'Discovery': 0.8888888888888888, 'Off-Topic': 0.7368421052631579, 'Troubleshooting': 0.8571428571428571}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.591991</td>\n",
       "      <td>0.823062</td>\n",
       "      <td>0.821729</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>{'Advice': 0.3076923076923077, 'Code': 0.4, 'Comparison': 0.8571428571428571, 'Discovery': 0.8994708994708994, 'Off-Topic': 0.631578947368421, 'Troubleshooting': 0.8888888888888888}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.772037</td>\n",
       "      <td>0.795688</td>\n",
       "      <td>0.806837</td>\n",
       "      <td>0.809211</td>\n",
       "      <td>{'Advice': 0.5, 'Code': 0.5882352941176471, 'Comparison': 0.7272727272727273, 'Discovery': 0.8756218905472637, 'Off-Topic': 0.4, 'Troubleshooting': 0.8333333333333334}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>1.162407</td>\n",
       "      <td>0.800764</td>\n",
       "      <td>0.799185</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>{'Advice': 0.0, 'Code': 0.4, 'Comparison': 0.8, 'Discovery': 0.8932038834951457, 'Off-Topic': 0.625, 'Troubleshooting': 0.88}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.866578</td>\n",
       "      <td>0.821323</td>\n",
       "      <td>0.807532</td>\n",
       "      <td>0.848684</td>\n",
       "      <td>{'Advice': 0.0, 'Code': 0.5333333333333333, 'Comparison': 0.9230769230769231, 'Discovery': 0.9054726368159204, 'Off-Topic': 0.5882352941176471, 'Troubleshooting': 0.9019607843137255}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.758480</td>\n",
       "      <td>0.834946</td>\n",
       "      <td>0.835680</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>{'Advice': 0.5, 'Code': 0.4, 'Comparison': 0.8333333333333334, 'Discovery': 0.90625, 'Off-Topic': 0.6666666666666666, 'Troubleshooting': 0.8727272727272727}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.839478</td>\n",
       "      <td>0.827312</td>\n",
       "      <td>0.831492</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>{'Advice': 0.4, 'Code': 0.42857142857142855, 'Comparison': 0.8333333333333334, 'Discovery': 0.898989898989899, 'Off-Topic': 0.5882352941176471, 'Troubleshooting': 0.9056603773584906}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.995142</td>\n",
       "      <td>0.842575</td>\n",
       "      <td>0.863645</td>\n",
       "      <td>0.861842</td>\n",
       "      <td>{'Advice': 0.4444444444444444, 'Code': 0.3076923076923077, 'Comparison': 1.0, 'Discovery': 0.9108910891089109, 'Off-Topic': 0.625, 'Troubleshooting': 0.9230769230769231}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.769416</td>\n",
       "      <td>0.845605</td>\n",
       "      <td>0.852713</td>\n",
       "      <td>0.855263</td>\n",
       "      <td>{'Advice': 0.6, 'Code': 0.42857142857142855, 'Comparison': 1.0, 'Discovery': 0.898989898989899, 'Off-Topic': 0.631578947368421, 'Troubleshooting': 0.9019607843137255}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.826104</td>\n",
       "      <td>0.860155</td>\n",
       "      <td>0.865225</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>{'Advice': 0.7272727272727273, 'Code': 0.42857142857142855, 'Comparison': 0.9230769230769231, 'Discovery': 0.9128205128205128, 'Off-Topic': 0.6666666666666666, 'Troubleshooting': 0.9056603773584906}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.817503</td>\n",
       "      <td>0.851414</td>\n",
       "      <td>0.859883</td>\n",
       "      <td>0.861842</td>\n",
       "      <td>{'Advice': 0.6, 'Code': 0.42857142857142855, 'Comparison': 0.9230769230769231, 'Discovery': 0.9081632653061225, 'Off-Topic': 0.6666666666666666, 'Troubleshooting': 0.9056603773584906}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.880502</td>\n",
       "      <td>0.860155</td>\n",
       "      <td>0.865225</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>{'Advice': 0.7272727272727273, 'Code': 0.42857142857142855, 'Comparison': 0.9230769230769231, 'Discovery': 0.9128205128205128, 'Off-Topic': 0.6666666666666666, 'Troubleshooting': 0.9056603773584906}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.877495</td>\n",
       "      <td>0.851414</td>\n",
       "      <td>0.859883</td>\n",
       "      <td>0.861842</td>\n",
       "      <td>{'Advice': 0.6, 'Code': 0.42857142857142855, 'Comparison': 0.9230769230769231, 'Discovery': 0.9081632653061225, 'Off-Topic': 0.6666666666666666, 'Troubleshooting': 0.9056603773584906}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.865381</td>\n",
       "      <td>0.851414</td>\n",
       "      <td>0.859883</td>\n",
       "      <td>0.861842</td>\n",
       "      <td>{'Advice': 0.6, 'Code': 0.42857142857142855, 'Comparison': 0.9230769230769231, 'Discovery': 0.9081632653061225, 'Off-Topic': 0.6666666666666666, 'Troubleshooting': 0.9056603773584906}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4dbd2-7e4c6e413acb0ddc37b63519;b1d97a2e-7b9e-4651-a3e9-6c4e40f4df87)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4dcec-5f1665633c5da40d7731a2b8;8d33ce65-52fe-464f-b8fa-a594dc25cd15)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4de0a-337e2d0023e30d866a4d9609;4cb1e66e-65bd-417f-852a-9984217c0941)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4df3b-0b95f6b303cf3a1b67c1124d;3c3de9e2-efd7-4a90-80ed-009520cfaa0f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e055-3eef8b246d1f334318aa00a1;dbca78ff-8cd7-432c-b053-65b7b5ccd24a)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e172-788f4b46392828bf7c11239e;356be1ff-2ecd-4680-ba57-0b58b7e0c42f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e28f-7f77eaee2b6ca040468f7f5c;5c2e9d3b-7c58-4c14-9c87-d5c6bd7e70b7)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e3c3-65b0c17e60740a4333a35694;af1d6d70-1620-454a-9f95-08fc2dbedb6f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e4de-56703a082c46e18a618d80e9;ca33d359-3c62-4ce9-8b31-dc38c8fdf770)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e5fe-7b8145923fef527028dc92c9;2ac81a40-cc46-48a4-b4eb-309eda2d889d)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e71a-55e0ff1f1e75b9b63a416980;f7c88bdc-e1fd-46ce-b4a6-3656cb1351f4)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e84e-5c5c8da15cf8d49e111a7351;71b9eac2-ca8e-40c4-99a5-5d2d586bd27f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e96c-5ab3cf014db28462003c5302;49386eae-f016-44e0-9d20-ed1328c196a4)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4ea8d-4c3b2ac207184dfb5f0e5999;62966556-733d-4ae6-a398-2596a29fca36)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4eba8-039778bb32310fb977bab9b8;ac8c23c4-6802-46cd-b312-6d97cd31f5d0)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1605, training_loss=0.29253894458307284, metrics={'train_runtime': 4348.6862, 'train_samples_per_second': 19.661, 'train_steps_per_second': 2.461, 'total_flos': 3.227800250173686e+17, 'train_loss': 0.29253894458307284, 'epoch': 15.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.config.update(config)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2e1840b97b4163a1043f9f10f3b837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_storage=torch.float16,\n",
    ")\n",
    "\n",
    "# model_id = \"meta-llama/Meta-Llama-3-70B\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=\"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\",  # add_prefix_space=True\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\n",
    "        \"quantization_config\": bnb_config,\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "        \"attn_implementation\": \"sdpa\",\n",
    "    },\n",
    "    device_map=\"auto\",\n",
    "    token=\"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\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2544/2384627403.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=nr_of_shots))\n"
     ]
    }
   ],
   "source": [
    "nr_of_shots = 2\n",
    "sampled_questions = (\n",
    "    train.groupby(\"label\")\n",
    "    .apply(lambda x: x.sample(n=nr_of_shots))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create a defaultdict with list as the default factory\n",
    "label_question_dict = defaultdict(list)\n",
    "\n",
    "# Iterate over the DataFrame rows and append questions to the appropriate label list\n",
    "for _, row in sampled_questions.iterrows():\n",
    "    label_question_dict[row[\"label\"]].append(row[\"question\"])\n",
    "\n",
    "# Convert defaultdict to a regular dictionary if desired\n",
    "label_question_dict = dict(label_question_dict)\n",
    "\n",
    "one_shot_examples = label_question_dict\n",
    "\n",
    "one_shot_examples_string = \"\"\n",
    "for label, questions in one_shot_examples.items():\n",
    "    for question in questions:\n",
    "        question = question.replace(\"Question: \", \"\")\n",
    "        one_shot_examples_string += f\"Question: {question}. Answer: {label}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"QUESTION TYPE\"\n",
    "target_with_brackets = f\"{{{target}}}\"\n",
    "labels = list(label2id.keys())\n",
    "label_token_lists = {}\n",
    "for l in list(label2id.keys()):\n",
    "    label_token_lists[l] = tokenizer.encode(l, add_special_tokens=False)\n",
    "\n",
    "\n",
    "def match_label(generated_string_tokens, ground_truth, label_token_lists):\n",
    "\n",
    "    def is_subsequence(subseq, seq):\n",
    "        \"\"\"\n",
    "        Check if subseq is a contiguous subsequence of seq.\n",
    "\n",
    "        Args:\n",
    "            subseq (list): The subsequence to check.\n",
    "            seq (list): The sequence to check against.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if subseq is a subsequence of seq, else False.\n",
    "        \"\"\"\n",
    "        sub_len = len(subseq)\n",
    "        for i in range(len(seq) - sub_len + 1):\n",
    "            if seq[i : i + sub_len] == subseq:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    matching_labels = []\n",
    "\n",
    "    # Check each label token list to see if it is a subsequence of the generated string tokens\n",
    "    for label, label_tokens in label_token_lists.items():\n",
    "        if is_subsequence(label_tokens, generated_string_tokens):\n",
    "            matching_labels.append(label)\n",
    "\n",
    "    # Return the label if only one matches, otherwise return None\n",
    "    if len(matching_labels) == 1 and matching_labels[0] == ground_truth:\n",
    "        return True  # matching_labels[0]\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "context = \"This question is asked by the user to our support tech team and you should classify it based on its type which represents the nature of the issue the user is encountering.\"\n",
    "\n",
    "task_info = \"\"\"This question asked by the user is of the following types:\n",
    "- `Discovery`: User is exploring how to do something or looking something up.\n",
    "- `Troubleshooting`: User is asking about an error, often this is a stacktrace.\n",
    "- `Code`: The user is inputing code and asking kapa to change, debug or explain something.\n",
    "- `Comparison`: User is asking kapa to contrast two things i.e. Are X and Y the same, what is the difference betwenn X and Y\n",
    "- `Advice`: The user is asking kapa what to do or what the best practice is.\n",
    "- `Off-Topic`: Anything else, could be something unrelated, gibberish, abuse and so on.\n",
    "\"\"\"\n",
    "\n",
    "few_shots = f\"\"\"Here are some examples of questions that are classified based on their type:\n",
    "\n",
    "{one_shot_examples_string}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def classify(question):\n",
    "    instructions = f\"\"\"\n",
    "    {context}\n",
    "    {task_info}\n",
    "    {few_shots}\n",
    "    \n",
    "    Return the {target} {labels} with one word/label response without any additional text. Answer: {target_with_brackets}.\"\"\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "    Question: {question}. Answer: \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "    ]\n",
    "    output = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=3,  # 64, 128, 256, 4096   #max_length\n",
    "        pad_token_id=pipeline.tokenizer.pad_token_id,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "        # top_k=1,\n",
    "        temperature=0.0,  # 0.1, 0.3, 0.6, 0.8\n",
    "        # top_p=0.9,  # 0.8, 0.9, 0.95\n",
    "        return_full_text=False,\n",
    "    )[0][\"generated_text\"].lstrip()\n",
    "    return output\n",
    "\n",
    "\n",
    "def final_output(question, ground_truth):\n",
    "    output = classify(question)\n",
    "    generated_string_tokens = tokenizer.encode(output, add_special_tokens=False)\n",
    "    if match_label(generated_string_tokens, ground_truth, label_token_lists):\n",
    "        return label2id[ground_truth]\n",
    "    else:\n",
    "        # choose random label from remaining ones excluding ground truth\n",
    "        remaining_labels = [l for l in labels if l != ground_truth]\n",
    "        random_label = np.random.choice(remaining_labels)\n",
    "        return label2id[random_label]\n",
    "\n",
    "\n",
    "# - which examples for few shot\n",
    "#     - based on embed similarity to the query\n",
    "#     - selected examples should be classified correctly via llama with zero shot\n",
    "# - read second use case again\n",
    "# instruct 3.1\n",
    "# optimize eval via dataset\n",
    "# if not working, try to run on same example 3 times and average result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/152 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 152/152 [02:50<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "questions = val[\"question\"].tolist()\n",
    "ground_truths = val[\"label\"].tolist()\n",
    "\n",
    "preds = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "for i in tqdm(range(len(questions))):\n",
    "    question = questions[i]\n",
    "    ground_truth = ground_truths[i]\n",
    "    pred = final_output(question, ground_truth)\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths_ids = [label2id[l] for l in ground_truths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.8908133975587537,\n",
       " 'precision': 0.9081554410501778,\n",
       " 'recall': 0.881578947368421,\n",
       " 'Advice': 0.7692307692307693,\n",
       " 'Code': 0.5555555555555556,\n",
       " 'Comparison': 0.5882352941176471,\n",
       " 'Discovery': 0.945054945054945,\n",
       " 'Off-Topic': 0.75,\n",
       " 'Troubleshooting': 0.96}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given preds and ground truth, calculate f1 score weighted\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.combine([\"f1\", \"precision\", \"recall\"])\n",
    "metrics = metric.compute(\n",
    "    predictions=preds, references=ground_truths_ids, average=\"weighted\"\n",
    ")\n",
    "# F1 Score per label\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "f1_per_label = f1_metric.compute(\n",
    "    predictions=preds, references=ground_truths_ids, average=None\n",
    ")\n",
    "f1_per_label_dict = {id2label[idx]: v for idx, v in enumerate(f1_per_label[\"f1\"])}\n",
    "\n",
    "metrics.update(f1_per_label_dict)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first 0 shot eval: {'f1': 0.16333863422483744,\n",
    " 'precision': 0.1188912318188634,\n",
    " 'recall': 0.27631578947368424,\n",
    " 'Advice': 0.42424242424242425,\n",
    " 'Code': 0.1875,\n",
    " 'Comparison': 0.25806451612903225,\n",
    " 'Discovery': 0.0,\n",
    " 'Off-Topic': 0.3684210526315789,\n",
    " 'Troubleshooting': 0.5675675675675675}\n",
    "\n",
    "\n",
    " with task info:\n",
    " {'f1': 0.7916982140666351,\n",
    " 'precision': 0.7932880300280484,\n",
    " 'recall': 0.7960526315789473,\n",
    " 'Advice': 0.5333333333333333,\n",
    " 'Code': 0.0,\n",
    " 'Comparison': 0.5333333333333333,\n",
    " 'Discovery': 0.9120879120879121,\n",
    " 'Off-Topic': 0.5384615384615384,\n",
    " 'Troubleshooting': 0.8363636363636363}\n",
    "\n",
    " version 1 with task info:\n",
    " {'f1': 0.8275137200685542,\n",
    " 'precision': 0.8454294091971503,\n",
    " 'recall': 0.8289473684210527,\n",
    " 'Advice': 0.5263157894736842,\n",
    " 'Code': 0.2,\n",
    " 'Comparison': 0.5,\n",
    " 'Discovery': 0.9398907103825137,\n",
    " 'Off-Topic': 0.6363636363636364,\n",
    " 'Troubleshooting': 0.8518518518518519}\n",
    "\n",
    "\n",
    "context: {'f1': 0.8308177333464014,\n",
    " 'precision': 0.8461722391385704,\n",
    " 'recall': 0.8223684210526315,\n",
    " 'Advice': 0.6666666666666666,\n",
    " 'Code': 0.125,\n",
    " 'Comparison': 0.5,\n",
    " 'Discovery': 0.9273743016759777,\n",
    " 'Off-Topic': 0.6666666666666666,\n",
    " 'Troubleshooting': 0.8888888888888888}\n",
    "\n",
    "\n",
    " one shot random:\n",
    " {'f1': 0.818100290285397,\n",
    " 'precision': 0.8498281729365859,\n",
    " 'recall': 0.8092105263157895,\n",
    " 'Advice': 0.6666666666666666,\n",
    " 'Code': 0.5,\n",
    " 'Comparison': 0.6666666666666666,\n",
    " 'Discovery': 0.8588235294117647,\n",
    " 'Off-Topic': 0.7407407407407407,\n",
    " 'Troubleshooting': 0.8771929824561403}\n",
    "\n",
    " 1 shot random\n",
    " {'f1': 0.891545941874889,\n",
    " 'precision': 0.9174498746867169,\n",
    " 'recall': 0.881578947368421,\n",
    " 'Advice': 0.75,\n",
    " 'Code': 0.625,\n",
    " 'Comparison': 0.6,\n",
    " 'Discovery': 0.9318181818181818,\n",
    " 'Off-Topic': 0.9090909090909091,\n",
    " 'Troubleshooting': 0.9259259259259259}\n",
    "\n",
    " 1 shot random: {'f1': 0.8651968615436109,\n",
    " 'precision': 0.8707967032967033,\n",
    " 'recall': 0.8618421052631579,\n",
    " 'Advice': 0.42857142857142855,\n",
    " 'Code': 0.375,\n",
    " 'Comparison': 0.6153846153846154,\n",
    " 'Discovery': 0.9513513513513514,\n",
    " 'Off-Topic': 0.72,\n",
    " 'Troubleshooting': 0.9411764705882353}\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 shot random:\n",
    "\n",
    "{'f1': 0.8908133975587537,\n",
    " 'precision': 0.9081554410501778,\n",
    " 'recall': 0.881578947368421,\n",
    " 'Advice': 0.7692307692307693,\n",
    " 'Code': 0.5555555555555556,\n",
    " 'Comparison': 0.5882352941176471,\n",
    " 'Discovery': 0.945054945054945,\n",
    " 'Off-Topic': 0.75,\n",
    " 'Troubleshooting': 0.96}\n",
    "\n",
    " {'Advice': ['Question: best way to integrate splunk forwarder into nxrm',\n",
    "  'Question:  How do I select a protocol for my IoT project?'],\n",
    " 'Code': [\"Question: Is this workflow signal handler valid?\\n    @workflow.signal\\n    async def add_user_message(self, phone_number: str, message: str) -> None:\\n        # Check if user messages already exist in the queue\\n        for user_messages in self._pending_user_messages._queue:\\n            if user_messages.phone_number == phone_number:\\n                user_messages.messages.append(message)\\n                break\\n        else:\\n            # If user messages don't exist, create a new UserMessages instance\\n            user_messages = MessageParams(user_id=phone_number, messages=[message])\\n            await self._pending_user_messages.put(user_messages)\",\n",
    "  'Question: this code is working good but there is an issue it show when the query is empty  @OnEvent(\"addToListInMonday\", { async: true })\\n    async handleAddMyListToMondayEvent(payload: AddToListPayloadType) {\\n        try {\\n            const query = `query {\\n  items(ids: [${payload.userMondayId}]) {\\n    column_values(ids: [\"${payload.connectedBoard}\"]) {\\n      value\\n    }\\n  }\\n}`;\\n\\n            const currentValues = await this.monday.api(query);\\n            console.log(currentValues.data.items[0].column_values[0].value)\\n            const currentCandidates = JSON.parse(currentValues.data.items[0].column_values[0].value).linkedPulseIds.map(item => item.linkedPulseId);\\n            currentCandidates.push(payload.candidateMondayId);\\n\\n\\n            const columnValues = {\\n                [payload.connectedBoard]: {\\n                    \"item_ids\": currentCandidates\\n                }\\n            };\\n\\n            const mutation = `mutation {\\n  change_multiple_column_values(item_id:${payload.userMondayId}, board_id:${payload.boardId}, column_values: \"${JSON.stringify(columnValues).replace(/\"/g, \\'\\\\\\\\\"\\')}\") {\\n    id\\n  }\\n}`;\\n            const res = await this.monday.api(mutation);\\n            if (!isNaN(res)) {\\n                return res\\n            }\\n        }\\n        catch (e) {\\n            console.log(e)\\n            return e\\n        }\\n    }{\"changed_at\":\"2024-03-23T01:00:18.227Z\"}\\nTypeError: Cannot read properties of undefined (reading \\'map\\')\\n    at MondayEvents.handleAddMyListToMondayEvent (C:\\\\Users\\\\Abdulaziz\\\\Desktop\\\\MyProjects\\\\iewa\\\\iewa-backend\\\\src\\\\Events\\\\Monday.events.ts:150:116)\\n    at processTicksAndRejections (node:internal/process/task_queues:95:5)\\n    at EventSubscribersLoader.wrapFunctionInTryCatchBlocks (C:\\\\'],\n",
    " 'Comparison': ['Question: What is the difference between stream position and commit position?',\n",
    "  'Question: difference between div and main'],\n",
    " 'Discovery': ['Question: Token ne zaman istenecek? (When will the token be requested?)',\n",
    "  'Question: Icons'],\n",
    " 'Off-Topic': ['Question: what is this in 10 words', 'Question: MANTà¥¤KUMAR'],\n",
    " 'Troubleshooting': ['Question: my portainer instance stopped connecting to agent on another instance. Both are on bu do not see each other',\n",
    "  'Question: I am not getting otp for login ']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
