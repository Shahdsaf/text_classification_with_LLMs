{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets accelerate scikit-learn evaluate wandb bitsandbytes peft --quiet\n",
    "!FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE pip install flash-attn --no-build-isolation --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation and splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Troubleshooting</td>\n",
       "      <td>Build Error\\nWhen I try to import an image sta...</td>\n",
       "      <td>Next.js handles static image imports slightly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>How can I specify my own character to separate...</td>\n",
       "      <td>I'm sorry, but based on the provided knowledge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>When can I buy Tari?</td>\n",
       "      <td>As of the latest updates, the Tari main net ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Off-Topic</td>\n",
       "      <td>Invest</td>\n",
       "      <td>I'm sorry, but your question is a bit vague. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>I there a way to define a global variable that...</td>\n",
       "      <td>Yes, in Mage, you can define global variables ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          category                                           question  \\\n",
       "0  Troubleshooting  Build Error\\nWhen I try to import an image sta...   \n",
       "1        Discovery  How can I specify my own character to separate...   \n",
       "2        Discovery                               When can I buy Tari?   \n",
       "3        Off-Topic                                             Invest   \n",
       "4        Discovery  I there a way to define a global variable that...   \n",
       "\n",
       "                                              answer  \n",
       "0  Next.js handles static image imports slightly ...  \n",
       "1  I'm sorry, but based on the provided knowledge...  \n",
       "2  As of the latest updates, the Tari main net ha...  \n",
       "3  I'm sorry, but your question is a bit vague. I...  \n",
       "4  Yes, in Mage, you can define global variables ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "with open(\"data.json\", \"r\") as f:\n",
    "    data = json.load(f)[\"data\"]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Discovery          734\n",
       "Troubleshooting    199\n",
       "Off-Topic           83\n",
       "Code                59\n",
       "Advice              53\n",
       "Comparison          50\n",
       "Off-topic            7\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Discovery          734\n",
       "Troubleshooting    199\n",
       "Off-Topic           90\n",
       "Code                59\n",
       "Advice              53\n",
       "Comparison          50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are 2 identical categories: \"Off-topic\" and \"Off-Topic\"\n",
    "# rename to \"Off-Topic\"\n",
    "\n",
    "df[\"category\"] = df[\"category\"].str.replace(\"Off-topic\", \"Off-Topic\")\n",
    "\n",
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Code</td>\n",
       "      <td>can you please convert this sample code in jav...</td>\n",
       "      <td>Sure, I can help you convert the JavaScript co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>how can i get mask all coordinate</td>\n",
       "      <td>Hmm, I don't know enough to give you a confide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>if a vulnerable package is already cached insi...</td>\n",
       "      <td>No, if a vulnerable package is already cached ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>how to get SMS for login</td>\n",
       "      <td>FusionAuth supports SMS-based multi-factor aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>Has anyone mentioned that the changelog or wee...</td>\n",
       "      <td>I'm sorry, but the provided knowledge sources ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                           question  \\\n",
       "72        Code  can you please convert this sample code in jav...   \n",
       "351  Discovery                  how can i get mask all coordinate   \n",
       "212  Discovery  if a vulnerable package is already cached insi...   \n",
       "17   Discovery                           how to get SMS for login   \n",
       "7    Discovery  Has anyone mentioned that the changelog or wee...   \n",
       "\n",
       "                                                answer  \n",
       "72   Sure, I can help you convert the JavaScript co...  \n",
       "351  Hmm, I don't know enough to give you a confide...  \n",
       "212  No, if a vulnerable package is already cached ...  \n",
       "17   FusionAuth supports SMS-based multi-factor aut...  \n",
       "7    I'm sorry, but the provided knowledge sources ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given labels are not equally distributed, split by category with stratify\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train, test = train_test_split(\n",
    "    df, test_size=0.15, stratify=df[\"category\"], random_state=42, shuffle=True\n",
    ")\n",
    "train, val = train_test_split(\n",
    "    train, test_size=0.15, stratify=train[\"category\"], random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index and save to parquet\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "# for each row, add to question column f\"Question: {row['question']}\" and answer column f\"Answer: {row['answer']}\" and create a new column combined question and answer in one column\n",
    "\n",
    "train[\"question\"] = train[\"question\"].apply(lambda x: f\"Question: {x}\")\n",
    "train[\"answer\"] = train[\"answer\"].apply(lambda x: f\"Answer: {x}\")\n",
    "\n",
    "val[\"question\"] = val[\"question\"].apply(lambda x: f\"Question: {x}\")\n",
    "val[\"answer\"] = val[\"answer\"].apply(lambda x: f\"Answer: {x}\")\n",
    "\n",
    "test[\"question\"] = test[\"question\"].apply(lambda x: f\"Question: {x}\")\n",
    "test[\"answer\"] = test[\"answer\"].apply(lambda x: f\"Answer: {x}\")\n",
    "\n",
    "train[\"combined\"] = train[\"question\"] + \"\\n\" + train[\"answer\"]\n",
    "val[\"combined\"] = val[\"question\"] + \"\\n\" + val[\"answer\"]\n",
    "test[\"combined\"] = test[\"question\"] + \"\\n\" + test[\"answer\"]\n",
    "\n",
    "\n",
    "train.to_parquet(\"train.parquet\")\n",
    "val.to_parquet(\"val.parquet\")\n",
    "test.to_parquet(\"test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Discovery          530\n",
       "Troubleshooting    143\n",
       "Off-Topic           65\n",
       "Code                42\n",
       "Advice              38\n",
       "Comparison          37\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read train and val from parquet\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train = pd.read_parquet(\"train.parquet\")\n",
    "val = pd.read_parquet(\"val.parquet\")\n",
    "test = pd.read_parquet(\"test.parquet\")\n",
    "\n",
    "# change column name from category to label\n",
    "train = train.rename(columns={\"category\": \"label\"})\n",
    "val = val.rename(columns={\"category\": \"label\"})\n",
    "test = test.rename(columns={\"category\": \"label\"})\n",
    "\n",
    "train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label       0\n",
      "question    0\n",
      "answer      0\n",
      "combined    0\n",
      "dtype: int64\n",
      "label       0\n",
      "question    0\n",
      "answer      0\n",
      "combined    0\n",
      "dtype: int64\n",
      "label       0\n",
      "question    0\n",
      "answer      0\n",
      "combined    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# get nan values for train and val and test\n",
    "\n",
    "print(train.isna().sum())\n",
    "print(val.isna().sum())\n",
    "print(test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>Question: OAPåœ¨Receiveræ¨¡å¼ä¸‹, å¿…é¡»è¦é…ç½®æˆé›†ç¾¤æ¨¡å¼å—?</td>\n",
       "      <td>Answer: æ˜¯çš„ï¼ŒOAPåœ¨Receiveræ¨¡å¼ä¸‹åº”è¯¥é…ç½®æˆé›†ç¾¤æ¨¡å¼ã€‚åœ¨é›†ç¾¤æ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰çš„...</td>\n",
       "      <td>Question: OAPåœ¨Receiveræ¨¡å¼ä¸‹, å¿…é¡»è¦é…ç½®æˆé›†ç¾¤æ¨¡å¼å—?\\nAnswe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Troubleshooting</td>\n",
       "      <td>Question: Build Error\\nWhen I try to import an...</td>\n",
       "      <td>Answer: Next.js handles static image imports s...</td>\n",
       "      <td>Question: Build Error\\nWhen I try to import an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Off-Topic</td>\n",
       "      <td>Question: Hello there</td>\n",
       "      <td>Answer: Hello! How can I assist you with Typef...</td>\n",
       "      <td>Question: Hello there\\nAnswer: Hello! How can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Off-Topic</td>\n",
       "      <td>Question: hi</td>\n",
       "      <td>Answer: Hello! How can I assist you with kapa....</td>\n",
       "      <td>Question: hi\\nAnswer: Hello! How can I assist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>Question: creating a new project in amplitude</td>\n",
       "      <td>Answer: Creating a new project in Amplitude in...</td>\n",
       "      <td>Question: creating a new project in amplitude ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label                                           question  \\\n",
       "0        Discovery            Question: OAPåœ¨Receiveræ¨¡å¼ä¸‹, å¿…é¡»è¦é…ç½®æˆé›†ç¾¤æ¨¡å¼å—?   \n",
       "1  Troubleshooting  Question: Build Error\\nWhen I try to import an...   \n",
       "2        Off-Topic                              Question: Hello there   \n",
       "3        Off-Topic                                       Question: hi   \n",
       "4        Discovery     Question: creating a new project in amplitude    \n",
       "\n",
       "                                              answer  \\\n",
       "0  Answer: æ˜¯çš„ï¼ŒOAPåœ¨Receiveræ¨¡å¼ä¸‹åº”è¯¥é…ç½®æˆé›†ç¾¤æ¨¡å¼ã€‚åœ¨é›†ç¾¤æ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰çš„...   \n",
       "1  Answer: Next.js handles static image imports s...   \n",
       "2  Answer: Hello! How can I assist you with Typef...   \n",
       "3  Answer: Hello! How can I assist you with kapa....   \n",
       "4  Answer: Creating a new project in Amplitude in...   \n",
       "\n",
       "                                            combined  \n",
       "0  Question: OAPåœ¨Receiveræ¨¡å¼ä¸‹, å¿…é¡»è¦é…ç½®æˆé›†ç¾¤æ¨¡å¼å—?\\nAnswe...  \n",
       "1  Question: Build Error\\nWhen I try to import an...  \n",
       "2  Question: Hello there\\nAnswer: Hello! How can ...  \n",
       "3  Question: hi\\nAnswer: Hello! How can I assist ...  \n",
       "4  Question: creating a new project in amplitude ...  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0769415cec439b80cce17e1190eefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/855 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a2ec1ce6724cd5ac947cff40e0fb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b4983518bc4db6b3b809b887ebcac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train, split=\"train\").class_encode_column(\"label\")\n",
    "class_label_feature = train_dataset.features[\"label\"]\n",
    "val_dataset = Dataset.from_pandas(val, split=\"val\").cast_column(\n",
    "    \"label\", class_label_feature\n",
    ")\n",
    "test_dataset = Dataset.from_pandas(test, split=\"test\").cast_column(\n",
    "    \"label\", class_label_feature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Advice',\n",
       " 1: 'Code',\n",
       " 2: 'Comparison',\n",
       " 3: 'Discovery',\n",
       " 4: 'Off-Topic',\n",
       " 5: 'Troubleshooting'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = train_dataset.features[\"label\"]._str2int\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# label2id = train_dataset.features['label'].names\n",
    "# id2label = {i: label for i, label in enumerate(label2id)}\n",
    "\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2595, 0.2348, 0.2665, 0.0186, 0.1517, 0.0690])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "label_id_df = train[\"label\"].map(label2id)\n",
    "\n",
    "class_weights = (1 / label_id_df.value_counts(normalize=True).sort_index()).tolist()\n",
    "class_weights = torch.tensor(class_weights)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_WATCH=all\n",
      "env: WANDB_SILENT=true\n",
      "env: WANDB_LOG_MODEL=false\n",
      "env: WANDB_PROJECT=kapa_question_type_classification\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_WATCH=all\n",
    "%env WANDB_SILENT=true\n",
    "#%env WANDB_LOG_MODEL=end\n",
    "%env WANDB_LOG_MODEL=false\n",
    "%env WANDB_PROJECT=kapa_question_type_classification\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"input_type\": \"question\",\n",
    "    \"version\": \"1-9\",\n",
    "    \"batch_size\": 16,\n",
    "    \"train_epochs\": 100,\n",
    "    \"num_workers\": 8,\n",
    "    \"lr\": 1e-4,\n",
    "    #\"dropout\": 0.3\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama tokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4d4736ac184b1fb6cbe335b4edc45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/855 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e202a4724344ec4b0af2cddde074323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb82c6aa26354e8991059e259878ba59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "if \"llama\" in config[\"model_name\"]:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        token=\"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\",\n",
    "        add_prefix_space=True,\n",
    "    )\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"llama tokenizer\")\n",
    "\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        token=\"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\",\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[config[\"input_type\"]], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Question: OAPåœ¨Receiveræ¨¡å¼ä¸‹, å¿…é¡»è¦é…ç½®æˆé›†ç¾¤æ¨¡å¼å—?\\nAnswer: æ˜¯çš„ï¼ŒOAPåœ¨Receiveræ¨¡å¼ä¸‹åº”è¯¥é…ç½®æˆé›†ç¾¤æ¨¡å¼ã€‚åœ¨é›†ç¾¤æ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰çš„OAPèŠ‚ç‚¹é»˜è®¤éƒ½åœ¨Mixedæ¨¡å¼ä¸‹è¿è¡Œã€‚ç„¶è€Œï¼Œæœ‰æ—¶ç”¨æˆ·å¯èƒ½å¸Œæœ›ä»¥æ˜ç¡®çš„è§’è‰²éƒ¨ç½²é›†ç¾¤èŠ‚ç‚¹ï¼Œä»–ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªåŠŸèƒ½ã€‚\\n\\nåœ¨Receiveræ¨¡å¼ä¸‹ï¼ŒOAPè´Ÿè´£ï¼š\\n1. æ¥æ”¶agentçš„è·Ÿè¸ªæˆ–æŒ‡æ ‡ã€‚\\n2. L1èšåˆ\\n3. å†…éƒ¨é€šä¿¡ï¼ˆå‘é€ï¼‰\\n\\nè¿™äº›è§’è‰²æ˜¯ä¸ºäº†æ»¡è¶³å®‰å…¨å’Œç½‘ç»œç­–ç•¥ä¸Šçš„å¤æ‚éƒ¨ç½²éœ€æ±‚è€Œè®¾è®¡çš„ã€‚å¦‚æœä½ æ­£åœ¨ä½¿ç”¨æˆ‘ä»¬çš„åŸç”Ÿ[Kubernetesåè°ƒå™¨](https://skywalking.apache.org/docs/main/latest/en/setup/backend-cluster#kubernetes)ï¼Œå¹¶ä¸”ä½ åšæŒè¦ä»¥æ˜ç¡®çš„è§’è‰²å®‰è£…OAPèŠ‚ç‚¹ã€‚é‚£ä¹ˆæ¯ä¸ªè§’è‰²åº”è¯¥æœ‰ä¸¤ä¸ªéƒ¨ç½²ï¼Œä¸€ä¸ªç”¨äºæ¥æ”¶å™¨OAPï¼Œå¦ä¸€ä¸ªç”¨äºèšåˆå™¨OAPï¼Œä»¥åˆ†éš”ä¸åŒçš„ç³»ç»Ÿç¯å¢ƒè®¾ç½®ã€‚ç„¶åï¼Œåº”è¯¥ä¸º`Aggregator`è§’è‰²é€‰æ‹©è§„åˆ™è®¾ç½®`labelSelector`ï¼Œä»¥æ ¹æ®ä½ çš„éœ€æ±‚é€‰æ‹©æ­£ç¡®çš„OAPéƒ¨ç½²ã€‚\\n\\nå‚è€ƒèµ„æ–™ï¼š[SkyWalking Advanced Deployment](https://skywalking.apache.org/docs/main/latest/en/setup/backend/advanced-deployment#advanced-deployment)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_test[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: OAPåœ¨Receiveræ¨¡å¼ä¸‹, å¿…é¡»è¦é…ç½®æˆé›†ç¾¤æ¨¡å¼å—?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test[0][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.combine([\"f1\", \"precision\", \"recall\"])\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    metrics = metric.compute(\n",
    "        predictions=predictions, references=labels, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    # F1 Score per label\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    f1_per_label = f1_metric.compute(\n",
    "        predictions=predictions, references=labels, average=None\n",
    "    )\n",
    "    f1_per_label_dict = {id2label[idx]: v for idx, v in enumerate(f1_per_label[\"f1\"])}\n",
    "\n",
    "    return {\n",
    "        \"f1\": metrics[\"f1\"],\n",
    "        \"precision\": metrics[\"precision\"],\n",
    "        \"recall\": metrics[\"recall\"],\n",
    "        \"f1_per_label\": f1_per_label_dict,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faeac4af927540ce89ad76d4686d8649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama model\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "if \"llama\" in config[\"model_name\"]:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        num_labels=6,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        token=\"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\",\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    print(\"llama model\")\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        num_labels=6,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        token=\"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\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# model.dropout = nn.Dropout(config[\"dropout\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13656064"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params\n",
    "\n",
    "\n",
    "count_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=4)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{config['model_name']}-v-{config['version']}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=config[\"lr\"],\n",
    "    per_device_train_batch_size=config[\"batch_size\"],\n",
    "    per_device_eval_batch_size=config[\"batch_size\"],\n",
    "    num_train_epochs=config[\"train_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    push_to_hub=False,\n",
    "    # fp16=True,\n",
    "    # fp16_full_eval=True,\n",
    "    bf16_full_eval=True,\n",
    "    bf16=True,\n",
    "    save_total_limit=1,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"wandb\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    run_name=f\"{config['model_name']}-v-{config['version']}\",\n",
    "    dataloader_num_workers=config[\"num_workers\"],\n",
    "    # gradient_accumulation_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(\n",
    "                self.args.device\n",
    "            )\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"label\").long()\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "# trainer = CustomTrainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=tokenized_train,\n",
    "#     eval_dataset=tokenized_val,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     callbacks=[early_stopping],\n",
    "#     data_collator=data_collator,\n",
    "#     class_weights=class_weights,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1605' max='10700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1605/10700 1:12:18 < 6:50:18, 0.37 it/s, Epoch 15/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Per Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.773900</td>\n",
       "      <td>1.114577</td>\n",
       "      <td>0.669231</td>\n",
       "      <td>0.717671</td>\n",
       "      <td>0.690789</td>\n",
       "      <td>{'Advice': 0.11764705882352941, 'Code': 0.2222222222222222, 'Comparison': 0.5, 'Discovery': 0.8076923076923077, 'Off-Topic': 0.47058823529411764, 'Troubleshooting': 0.5777777777777777}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.756500</td>\n",
       "      <td>0.831966</td>\n",
       "      <td>0.792158</td>\n",
       "      <td>0.783520</td>\n",
       "      <td>0.822368</td>\n",
       "      <td>{'Advice': 0.0, 'Code': 0.3076923076923077, 'Comparison': 0.6666666666666666, 'Discovery': 0.8888888888888888, 'Off-Topic': 0.7368421052631579, 'Troubleshooting': 0.8571428571428571}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.591991</td>\n",
       "      <td>0.823062</td>\n",
       "      <td>0.821729</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>{'Advice': 0.3076923076923077, 'Code': 0.4, 'Comparison': 0.8571428571428571, 'Discovery': 0.8994708994708994, 'Off-Topic': 0.631578947368421, 'Troubleshooting': 0.8888888888888888}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.772037</td>\n",
       "      <td>0.795688</td>\n",
       "      <td>0.806837</td>\n",
       "      <td>0.809211</td>\n",
       "      <td>{'Advice': 0.5, 'Code': 0.5882352941176471, 'Comparison': 0.7272727272727273, 'Discovery': 0.8756218905472637, 'Off-Topic': 0.4, 'Troubleshooting': 0.8333333333333334}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>1.162407</td>\n",
       "      <td>0.800764</td>\n",
       "      <td>0.799185</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>{'Advice': 0.0, 'Code': 0.4, 'Comparison': 0.8, 'Discovery': 0.8932038834951457, 'Off-Topic': 0.625, 'Troubleshooting': 0.88}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.866578</td>\n",
       "      <td>0.821323</td>\n",
       "      <td>0.807532</td>\n",
       "      <td>0.848684</td>\n",
       "      <td>{'Advice': 0.0, 'Code': 0.5333333333333333, 'Comparison': 0.9230769230769231, 'Discovery': 0.9054726368159204, 'Off-Topic': 0.5882352941176471, 'Troubleshooting': 0.9019607843137255}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.758480</td>\n",
       "      <td>0.834946</td>\n",
       "      <td>0.835680</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>{'Advice': 0.5, 'Code': 0.4, 'Comparison': 0.8333333333333334, 'Discovery': 0.90625, 'Off-Topic': 0.6666666666666666, 'Troubleshooting': 0.8727272727272727}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.839478</td>\n",
       "      <td>0.827312</td>\n",
       "      <td>0.831492</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>{'Advice': 0.4, 'Code': 0.42857142857142855, 'Comparison': 0.8333333333333334, 'Discovery': 0.898989898989899, 'Off-Topic': 0.5882352941176471, 'Troubleshooting': 0.9056603773584906}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.995142</td>\n",
       "      <td>0.842575</td>\n",
       "      <td>0.863645</td>\n",
       "      <td>0.861842</td>\n",
       "      <td>{'Advice': 0.4444444444444444, 'Code': 0.3076923076923077, 'Comparison': 1.0, 'Discovery': 0.9108910891089109, 'Off-Topic': 0.625, 'Troubleshooting': 0.9230769230769231}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.769416</td>\n",
       "      <td>0.845605</td>\n",
       "      <td>0.852713</td>\n",
       "      <td>0.855263</td>\n",
       "      <td>{'Advice': 0.6, 'Code': 0.42857142857142855, 'Comparison': 1.0, 'Discovery': 0.898989898989899, 'Off-Topic': 0.631578947368421, 'Troubleshooting': 0.9019607843137255}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.826104</td>\n",
       "      <td>0.860155</td>\n",
       "      <td>0.865225</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>{'Advice': 0.7272727272727273, 'Code': 0.42857142857142855, 'Comparison': 0.9230769230769231, 'Discovery': 0.9128205128205128, 'Off-Topic': 0.6666666666666666, 'Troubleshooting': 0.9056603773584906}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.817503</td>\n",
       "      <td>0.851414</td>\n",
       "      <td>0.859883</td>\n",
       "      <td>0.861842</td>\n",
       "      <td>{'Advice': 0.6, 'Code': 0.42857142857142855, 'Comparison': 0.9230769230769231, 'Discovery': 0.9081632653061225, 'Off-Topic': 0.6666666666666666, 'Troubleshooting': 0.9056603773584906}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.880502</td>\n",
       "      <td>0.860155</td>\n",
       "      <td>0.865225</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>{'Advice': 0.7272727272727273, 'Code': 0.42857142857142855, 'Comparison': 0.9230769230769231, 'Discovery': 0.9128205128205128, 'Off-Topic': 0.6666666666666666, 'Troubleshooting': 0.9056603773584906}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.877495</td>\n",
       "      <td>0.851414</td>\n",
       "      <td>0.859883</td>\n",
       "      <td>0.861842</td>\n",
       "      <td>{'Advice': 0.6, 'Code': 0.42857142857142855, 'Comparison': 0.9230769230769231, 'Discovery': 0.9081632653061225, 'Off-Topic': 0.6666666666666666, 'Troubleshooting': 0.9056603773584906}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.865381</td>\n",
       "      <td>0.851414</td>\n",
       "      <td>0.859883</td>\n",
       "      <td>0.861842</td>\n",
       "      <td>{'Advice': 0.6, 'Code': 0.42857142857142855, 'Comparison': 0.9230769230769231, 'Discovery': 0.9081632653061225, 'Off-Topic': 0.6666666666666666, 'Troubleshooting': 0.9056603773584906}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4dbd2-7e4c6e413acb0ddc37b63519;b1d97a2e-7b9e-4651-a3e9-6c4e40f4df87)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4dcec-5f1665633c5da40d7731a2b8;8d33ce65-52fe-464f-b8fa-a594dc25cd15)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4de0a-337e2d0023e30d866a4d9609;4cb1e66e-65bd-417f-852a-9984217c0941)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4df3b-0b95f6b303cf3a1b67c1124d;3c3de9e2-efd7-4a90-80ed-009520cfaa0f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e055-3eef8b246d1f334318aa00a1;dbca78ff-8cd7-432c-b053-65b7b5ccd24a)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e172-788f4b46392828bf7c11239e;356be1ff-2ecd-4680-ba57-0b58b7e0c42f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e28f-7f77eaee2b6ca040468f7f5c;5c2e9d3b-7c58-4c14-9c87-d5c6bd7e70b7)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e3c3-65b0c17e60740a4333a35694;af1d6d70-1620-454a-9f95-08fc2dbedb6f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e4de-56703a082c46e18a618d80e9;ca33d359-3c62-4ce9-8b31-dc38c8fdf770)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e5fe-7b8145923fef527028dc92c9;2ac81a40-cc46-48a4-b4eb-309eda2d889d)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e71a-55e0ff1f1e75b9b63a416980;f7c88bdc-e1fd-46ce-b4a6-3656cb1351f4)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e84e-5c5c8da15cf8d49e111a7351;71b9eac2-ca8e-40c4-99a5-5d2d586bd27f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4e96c-5ab3cf014db28462003c5302;49386eae-f016-44e0-9d20-ed1328c196a4)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4ea8d-4c3b2ac207184dfb5f0e5999;62966556-733d-4ae6-a398-2596a29fca36)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4eba8-039778bb32310fb977bab9b8;ac8c23c4-6802-46cd-b312-6d97cd31f5d0)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1605, training_loss=0.29253894458307284, metrics={'train_runtime': 4348.6862, 'train_samples_per_second': 19.661, 'train_steps_per_second': 2.461, 'total_flos': 3.227800250173686e+17, 'train_loss': 0.29253894458307284, 'epoch': 15.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.config.update(config)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92efa4ce7e4b4dfc97ac45acb7bbea4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2f19cb278941e4be35d4873b9e413e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00022-of-00030.safetensors:  22%|##2       | 1.04G/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403fbcc278a74152a1f08599e6887aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00023-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bff266903945749029aa38427d2c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00024-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b0fe5eca454112b8b1e0d7690626fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00025-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce854178414403fb78e779de42c32d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00026-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff921e784e44455993923f6c2f6e9abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00027-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4a6f450e6c4adf873c345dead418fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00028-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e0aa5f779d4419ab58eb967818be99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00029-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f28e61541a240b58b321f25cec2860d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00030-of-00030.safetensors:   0%|          | 0.00/2.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dd9693ea304fca94a23e2ecd9d4f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e098b22d0d4f00837cdc021ea65f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_storage=torch.float16,\n",
    ")\n",
    "\n",
    "# model_id = \"meta-llama/Meta-Llama-3-70B\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=\"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\",\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\n",
    "        \"quantization_config\": bnb_config,\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "        \"attn_implementation\": \"sdpa\",\n",
    "    },\n",
    "    device_map=\"auto\",\n",
    "    token=\"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\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Advice': [80039],\n",
       " 'Code': [2123],\n",
       " 'Comparison': [34587],\n",
       " 'Discovery': [68500],\n",
       " 'Off-Topic': [4699, 12, 27504],\n",
       " 'Troubleshooting': [91635, 65, 51340]}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def match_label(generated_string_tokens, label_token_lists):\n",
    "    \"\"\"\n",
    "    Check if exactly one label's token IDs are a subsequence of the generated string's token IDs.\n",
    "    If so, return that label's token list. If multiple labels are found as subsequences, return None.\n",
    "\n",
    "    Args:\n",
    "        generated_string_tokens (list): List of token IDs for the generated string.\n",
    "        label_token_lists (dict of lists): Dict of token ID lists, where each sublist represents a label.\n",
    "\n",
    "    Returns:\n",
    "        list or None: The label's token list if exactly one is found, otherwise None.\n",
    "    \"\"\"\n",
    "\n",
    "    def is_subsequence(subseq, seq):\n",
    "        \"\"\"\n",
    "        Check if subseq is a contiguous subsequence of seq.\n",
    "\n",
    "        Args:\n",
    "            subseq (list): The subsequence to check.\n",
    "            seq (list): The sequence to check against.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if subseq is a subsequence of seq, else False.\n",
    "        \"\"\"\n",
    "        sub_len = len(subseq)\n",
    "        for i in range(len(seq) - sub_len + 1):\n",
    "            if seq[i : i + sub_len] == subseq:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    matching_labels = []\n",
    "\n",
    "    # Check each label token list to see if it is a subsequence of the generated string tokens\n",
    "    for label, label_tokens in label_token_lists.items():\n",
    "        if is_subsequence(label_tokens, generated_string_tokens):\n",
    "            matching_labels.append(label)\n",
    "\n",
    "    # Return the label if only one matches, otherwise return None\n",
    "    if len(matching_labels) == 1:\n",
    "        return True  # matching_labels[0]\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "label_token_lists = {}\n",
    "for l in list(label2id.keys()):\n",
    "    label_token_lists[l] = tokenizer.encode(l, add_special_tokens=False)\n",
    "label_token_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovery\n",
      " Code.\n"
     ]
    }
   ],
   "source": [
    "sample = train.sample(1)\n",
    "question = sample[\"question\"].iloc[0]\n",
    "ground_truth = sample[\"label\"].iloc[0]\n",
    "print(ground_truth)\n",
    "\n",
    "target = \"QUESTION TYPE\"\n",
    "target_with_brackets = f\"{{{target}}}\"\n",
    "\n",
    "labels = list(label2id.keys())\n",
    "\n",
    "text = f\"\"\"\n",
    "Return the {target} {labels} with one word/label response without any additional text. Answer: {target_with_brackets}.\n",
    "\n",
    "Question: {question}. Answer: \"\"\"\n",
    "\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "output = pipeline(\n",
    "    text,\n",
    "    max_new_tokens=3,  # 64, 128, 256, 4096   #max_length\n",
    "    pad_token_id=pipeline.tokenizer.pad_token_id,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=False,\n",
    "    # top_k=1,\n",
    "    temperature=0.0,  # 0.1, 0.3, 0.6, 0.8\n",
    "    # top_p=0.9,  # 0.8, 0.9, 0.95\n",
    "    return_full_text=False,\n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "print(output)\n",
    "\n",
    "# formatting to include task info, context to experiment with all 3 formats\n",
    "# notes on notion\n",
    "# exp with zero shot across different examples and labels ... if works, evaluate on val set\n",
    "# if not working, try to run on same example 3 times and average result.\n",
    "# few shot testing ... nr of shots per label\n",
    "# instruct 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_string_tokens = tokenizer.encode(output, add_special_tokens=False)\n",
    "\n",
    "\n",
    "match_label(generated_string_tokens, label_token_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
