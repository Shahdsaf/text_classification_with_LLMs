{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets accelerate scikit-learn evaluate wandb bitsandbytes peft --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation and splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Troubleshooting</td>\n",
       "      <td>Build Error\\nWhen I try to import an image sta...</td>\n",
       "      <td>Next.js handles static image imports slightly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>How can I specify my own character to separate...</td>\n",
       "      <td>I'm sorry, but based on the provided knowledge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>When can I buy Tari?</td>\n",
       "      <td>As of the latest updates, the Tari main net ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Off-Topic</td>\n",
       "      <td>Invest</td>\n",
       "      <td>I'm sorry, but your question is a bit vague. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>I there a way to define a global variable that...</td>\n",
       "      <td>Yes, in Mage, you can define global variables ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          category                                           question  \\\n",
       "0  Troubleshooting  Build Error\\nWhen I try to import an image sta...   \n",
       "1        Discovery  How can I specify my own character to separate...   \n",
       "2        Discovery                               When can I buy Tari?   \n",
       "3        Off-Topic                                             Invest   \n",
       "4        Discovery  I there a way to define a global variable that...   \n",
       "\n",
       "                                              answer  \n",
       "0  Next.js handles static image imports slightly ...  \n",
       "1  I'm sorry, but based on the provided knowledge...  \n",
       "2  As of the latest updates, the Tari main net ha...  \n",
       "3  I'm sorry, but your question is a bit vague. I...  \n",
       "4  Yes, in Mage, you can define global variables ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "with open(\"data.json\", \"r\") as f:\n",
    "    data = json.load(f)[\"data\"]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Discovery          734\n",
       "Troubleshooting    199\n",
       "Off-Topic           83\n",
       "Code                59\n",
       "Advice              53\n",
       "Comparison          50\n",
       "Off-topic            7\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Discovery          734\n",
       "Troubleshooting    199\n",
       "Off-Topic           90\n",
       "Code                59\n",
       "Advice              53\n",
       "Comparison          50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are 2 identical categories: \"Off-topic\" and \"Off-Topic\"\n",
    "# rename to \"Off-Topic\"\n",
    "\n",
    "df[\"category\"] = df[\"category\"].str.replace(\"Off-topic\", \"Off-Topic\")\n",
    "\n",
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Code</td>\n",
       "      <td>can you please convert this sample code in jav...</td>\n",
       "      <td>Sure, I can help you convert the JavaScript co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>how can i get mask all coordinate</td>\n",
       "      <td>Hmm, I don't know enough to give you a confide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>if a vulnerable package is already cached insi...</td>\n",
       "      <td>No, if a vulnerable package is already cached ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>how to get SMS for login</td>\n",
       "      <td>FusionAuth supports SMS-based multi-factor aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>Has anyone mentioned that the changelog or wee...</td>\n",
       "      <td>I'm sorry, but the provided knowledge sources ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                           question  \\\n",
       "72        Code  can you please convert this sample code in jav...   \n",
       "351  Discovery                  how can i get mask all coordinate   \n",
       "212  Discovery  if a vulnerable package is already cached insi...   \n",
       "17   Discovery                           how to get SMS for login   \n",
       "7    Discovery  Has anyone mentioned that the changelog or wee...   \n",
       "\n",
       "                                                answer  \n",
       "72   Sure, I can help you convert the JavaScript co...  \n",
       "351  Hmm, I don't know enough to give you a confide...  \n",
       "212  No, if a vulnerable package is already cached ...  \n",
       "17   FusionAuth supports SMS-based multi-factor aut...  \n",
       "7    I'm sorry, but the provided knowledge sources ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given labels are not equally distributed, split by category with stratify\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train, test = train_test_split(\n",
    "    df, test_size=0.15, stratify=df[\"category\"], random_state=42, shuffle=True\n",
    ")\n",
    "train, val = train_test_split(\n",
    "    train, test_size=0.15, stratify=train[\"category\"], random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index and save to parquet\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "# for each row, add to question column f\"Question: {row['question']}\" and answer column f\"Answer: {row['answer']}\" and create a new column combined question and answer in one column\n",
    "\n",
    "train[\"question\"] = train[\"question\"].apply(lambda x: f\"Question: {x}\")\n",
    "train[\"answer\"] = train[\"answer\"].apply(lambda x: f\"Answer: {x}\")\n",
    "\n",
    "val[\"question\"] = val[\"question\"].apply(lambda x: f\"Question: {x}\")\n",
    "val[\"answer\"] = val[\"answer\"].apply(lambda x: f\"Answer: {x}\")\n",
    "\n",
    "test[\"question\"] = test[\"question\"].apply(lambda x: f\"Question: {x}\")\n",
    "test[\"answer\"] = test[\"answer\"].apply(lambda x: f\"Answer: {x}\")\n",
    "\n",
    "train[\"combined\"] = train[\"question\"] + \"\\n\" + train[\"answer\"]\n",
    "val[\"combined\"] = val[\"question\"] + \"\\n\" + val[\"answer\"]\n",
    "test[\"combined\"] = test[\"question\"] + \"\\n\" + test[\"answer\"]\n",
    "\n",
    "\n",
    "train.to_parquet(\"train.parquet\")\n",
    "val.to_parquet(\"val.parquet\")\n",
    "test.to_parquet(\"test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Discovery          530\n",
       "Troubleshooting    143\n",
       "Off-Topic           65\n",
       "Code                42\n",
       "Advice              38\n",
       "Comparison          37\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read train and val from parquet\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train = pd.read_parquet(\"train.parquet\")\n",
    "val = pd.read_parquet(\"val.parquet\")\n",
    "test = pd.read_parquet(\"test.parquet\")\n",
    "\n",
    "# change column name from category to label\n",
    "train = train.rename(columns={\"category\": \"label\"})\n",
    "val = val.rename(columns={\"category\": \"label\"})\n",
    "test = test.rename(columns={\"category\": \"label\"})\n",
    "\n",
    "train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label       0\n",
      "question    0\n",
      "answer      0\n",
      "combined    0\n",
      "dtype: int64\n",
      "label       0\n",
      "question    0\n",
      "answer      0\n",
      "combined    0\n",
      "dtype: int64\n",
      "label       0\n",
      "question    0\n",
      "answer      0\n",
      "combined    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# get nan values for train and val and test\n",
    "\n",
    "print(train.isna().sum())\n",
    "print(val.isna().sum())\n",
    "print(test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>Question: OAPÂú®ReceiverÊ®°Âºè‰∏ã, ÂøÖÈ°ªË¶ÅÈÖçÁΩÆÊàêÈõÜÁæ§Ê®°ÂºèÂêó?</td>\n",
       "      <td>Answer: ÊòØÁöÑÔºåOAPÂú®ReceiverÊ®°Âºè‰∏ãÂ∫îËØ•ÈÖçÁΩÆÊàêÈõÜÁæ§Ê®°Âºè„ÄÇÂú®ÈõÜÁæ§Ê®°Âºè‰∏ãÔºåÊâÄÊúâÁöÑ...</td>\n",
       "      <td>Question: OAPÂú®ReceiverÊ®°Âºè‰∏ã, ÂøÖÈ°ªË¶ÅÈÖçÁΩÆÊàêÈõÜÁæ§Ê®°ÂºèÂêó?\\nAnswe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Troubleshooting</td>\n",
       "      <td>Question: Build Error\\nWhen I try to import an...</td>\n",
       "      <td>Answer: Next.js handles static image imports s...</td>\n",
       "      <td>Question: Build Error\\nWhen I try to import an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Off-Topic</td>\n",
       "      <td>Question: Hello there</td>\n",
       "      <td>Answer: Hello! How can I assist you with Typef...</td>\n",
       "      <td>Question: Hello there\\nAnswer: Hello! How can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Off-Topic</td>\n",
       "      <td>Question: hi</td>\n",
       "      <td>Answer: Hello! How can I assist you with kapa....</td>\n",
       "      <td>Question: hi\\nAnswer: Hello! How can I assist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>Question: creating a new project in amplitude</td>\n",
       "      <td>Answer: Creating a new project in Amplitude in...</td>\n",
       "      <td>Question: creating a new project in amplitude ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label                                           question  \\\n",
       "0        Discovery            Question: OAPÂú®ReceiverÊ®°Âºè‰∏ã, ÂøÖÈ°ªË¶ÅÈÖçÁΩÆÊàêÈõÜÁæ§Ê®°ÂºèÂêó?   \n",
       "1  Troubleshooting  Question: Build Error\\nWhen I try to import an...   \n",
       "2        Off-Topic                              Question: Hello there   \n",
       "3        Off-Topic                                       Question: hi   \n",
       "4        Discovery     Question: creating a new project in amplitude    \n",
       "\n",
       "                                              answer  \\\n",
       "0  Answer: ÊòØÁöÑÔºåOAPÂú®ReceiverÊ®°Âºè‰∏ãÂ∫îËØ•ÈÖçÁΩÆÊàêÈõÜÁæ§Ê®°Âºè„ÄÇÂú®ÈõÜÁæ§Ê®°Âºè‰∏ãÔºåÊâÄÊúâÁöÑ...   \n",
       "1  Answer: Next.js handles static image imports s...   \n",
       "2  Answer: Hello! How can I assist you with Typef...   \n",
       "3  Answer: Hello! How can I assist you with kapa....   \n",
       "4  Answer: Creating a new project in Amplitude in...   \n",
       "\n",
       "                                            combined  \n",
       "0  Question: OAPÂú®ReceiverÊ®°Âºè‰∏ã, ÂøÖÈ°ªË¶ÅÈÖçÁΩÆÊàêÈõÜÁæ§Ê®°ÂºèÂêó?\\nAnswe...  \n",
       "1  Question: Build Error\\nWhen I try to import an...  \n",
       "2  Question: Hello there\\nAnswer: Hello! How can ...  \n",
       "3  Question: hi\\nAnswer: Hello! How can I assist ...  \n",
       "4  Question: creating a new project in amplitude ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340d2898f3064c3ca397c02806db23bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/855 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f4581466154a619334920d293dc7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbec2b30fc14f02b55fa2def5f6276b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "label2id = {\n",
    "    \"Discovery\": 0,\n",
    "    \"Troubleshooting\": 1,\n",
    "    \"Off-Topic\": 2,\n",
    "    \"Code\": 3,\n",
    "    \"Advice\": 4,\n",
    "    \"Comparison\": 5,\n",
    "}\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train, split=\"train\").class_encode_column(\"label\")\n",
    "class_label_feature = train_dataset.features[\"label\"]\n",
    "val_dataset = Dataset.from_pandas(val, split=\"val\").cast_column(\n",
    "    \"label\", class_label_feature\n",
    ")\n",
    "test_dataset = Dataset.from_pandas(test, split=\"test\").cast_column(\n",
    "    \"label\", class_label_feature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Advice',\n",
       " 1: 'Code',\n",
       " 2: 'Comparison',\n",
       " 3: 'Discovery',\n",
       " 4: 'Off-Topic',\n",
       " 5: 'Troubleshooting'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = train_dataset.features[\"label\"]._str2int\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_WATCH=all\n",
      "env: WANDB_SILENT=true\n",
      "env: WANDB_LOG_MODEL=end\n",
      "env: WANDB_PROJECT=kapa_question_type_classification\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_WATCH=all\n",
    "%env WANDB_SILENT=true\n",
    "%env WANDB_LOG_MODEL=end\n",
    "%env WANDB_PROJECT=kapa_question_type_classification\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"input_type\": \"question\",\n",
    "    \"version\": \"1\",\n",
    "    \"batch_size\": 16,\n",
    "    \"train_epochs\": 100,\n",
    "    \"num_workers\": 8,\n",
    "    \"lr\": 1e-5 ,\n",
    "    #\"dropout\": 0.3\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f438d0a81f4f52ae71e5cac639aac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/855 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f5dfa314644aa8a50fd1a3a0c4a18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10073766cea04372a494cd1ea50cef3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "if \"llama\" in config[\"model_name\"]:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        token=\"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\",\n",
    "        add_prefix_space=True,\n",
    "    )\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        token=\"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\",\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[config[\"input_type\"]], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Question: OAPÂú®ReceiverÊ®°Âºè‰∏ã, ÂøÖÈ°ªË¶ÅÈÖçÁΩÆÊàêÈõÜÁæ§Ê®°ÂºèÂêó?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_test[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: OAPÂú®ReceiverÊ®°Âºè‰∏ã, ÂøÖÈ°ªË¶ÅÈÖçÁΩÆÊàêÈõÜÁæ§Ê®°ÂºèÂêó?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test[0][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.combine([\"f1\", \"precision\", \"recall\"])\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    metrics = metric.compute(\n",
    "        predictions=predictions, references=labels, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    # F1 Score per label\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    f1_per_label = f1_metric.compute(\n",
    "        predictions=predictions, references=labels, average=None\n",
    "    )\n",
    "    f1_per_label_dict = {id2label[idx]: v for idx, v in enumerate(f1_per_label[\"f1\"])}\n",
    "\n",
    "    return {\n",
    "        \"f1\": metrics[\"f1\"],\n",
    "        \"precision\": metrics[\"precision\"],\n",
    "        \"recall\": metrics[\"recall\"],\n",
    "        \"f1_per_label\": f1_per_label_dict,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debd428428214449ba349cf367d1aad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "if \"llama\" in config[\"model_name\"]:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        num_labels=6,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        token=\"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\",\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        num_labels=6,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        token=\"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\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# model.dropout = nn.Dropout(config[\"dropout\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13656064"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params\n",
    "\n",
    "\n",
    "count_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=7)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{config['model_name']}-v-{config['version']}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=config[\"lr\"],\n",
    "    per_device_train_batch_size=config[\"batch_size\"],\n",
    "    per_device_eval_batch_size=config[\"batch_size\"],\n",
    "    num_train_epochs=config[\"train_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    push_to_hub=False,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    # bf16_full_eval=True,\n",
    "    # bf16=True,\n",
    "    save_total_limit=1,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"wandb\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    run_name=f\"{config['model_name']}-v-{config['version']}\",\n",
    "    dataloader_num_workers=config[\"num_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='430' max='5400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 430/5400 14:38 < 2:50:05, 0.49 it/s, Epoch 7.94/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Per Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.353600</td>\n",
       "      <td>2.517055</td>\n",
       "      <td>0.352346</td>\n",
       "      <td>0.411936</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>{'Advice': 0.0, 'Code': 0.12903225806451613, 'Comparison': 0.10526315789473684, 'Discovery': 0.524390243902439, 'Off-Topic': 0.0, 'Troubleshooting': 0.1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.566300</td>\n",
       "      <td>1.958161</td>\n",
       "      <td>0.493073</td>\n",
       "      <td>0.457018</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>{'Advice': 0.0, 'Code': 0.2222222222222222, 'Comparison': 0.0, 'Discovery': 0.7476635514018691, 'Off-Topic': 0.0, 'Troubleshooting': 0.1111111111111111}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.998500</td>\n",
       "      <td>1.663658</td>\n",
       "      <td>0.571038</td>\n",
       "      <td>0.551156</td>\n",
       "      <td>0.611842</td>\n",
       "      <td>{'Advice': 0.0, 'Code': 0.26666666666666666, 'Comparison': 0.0, 'Discovery': 0.780952380952381, 'Off-Topic': 0.23529411764705882, 'Troubleshooting': 0.3333333333333333}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.558000</td>\n",
       "      <td>1.445163</td>\n",
       "      <td>0.614202</td>\n",
       "      <td>0.601202</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>{'Advice': 0.0, 'Code': 0.15384615384615385, 'Comparison': 0.18181818181818182, 'Discovery': 0.8075117370892019, 'Off-Topic': 0.25, 'Troubleshooting': 0.47619047619047616}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.186200</td>\n",
       "      <td>1.259983</td>\n",
       "      <td>0.649441</td>\n",
       "      <td>0.636122</td>\n",
       "      <td>0.690789</td>\n",
       "      <td>{'Advice': 0.0, 'Code': 0.3076923076923077, 'Comparison': 0.2222222222222222, 'Discovery': 0.8301886792452831, 'Off-Topic': 0.35294117647058826, 'Troubleshooting': 0.5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.881000</td>\n",
       "      <td>1.145774</td>\n",
       "      <td>0.675524</td>\n",
       "      <td>0.662727</td>\n",
       "      <td>0.703947</td>\n",
       "      <td>{'Advice': 0.0, 'Code': 0.46153846153846156, 'Comparison': 0.4, 'Discovery': 0.8252427184466019, 'Off-Topic': 0.42105263157894735, 'Troubleshooting': 0.5531914893617021}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.654300</td>\n",
       "      <td>1.066433</td>\n",
       "      <td>0.689768</td>\n",
       "      <td>0.678642</td>\n",
       "      <td>0.717105</td>\n",
       "      <td>{'Advice': 0.0, 'Code': 0.46153846153846156, 'Comparison': 0.36363636363636365, 'Discovery': 0.8349514563106796, 'Off-Topic': 0.5263157894736842, 'Troubleshooting': 0.5652173913043478}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4205e-50cc685d57454cb80e99a45e;fa9c2169-4eeb-434c-98ce-6f30e815cf5f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b420ca-4f54985b7a1d27ee6aaeeb3f;675f7f33-83e8-4314-9243-c3e336e2df72)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4213a-47771dde13ef93ce17f4caf5;e3ad8964-8f68-462c-96f4-c38eb0eb9b6b)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b421a7-151669943f9a3e6962f5f2a8;d2efdf14-a2a8-4bd1-86a8-84e00ace4c11)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b42218-21e2c50b6ae0634722431937;7c408d31-0b55-437e-b0d5-688d6a439c99)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b4228a-6119d24754ebfa3c126e88ef;84cb8785-2002-45d7-876e-cb96dcd5c4dc)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66b422fc-5d30fa413b0702b038c282d2;ed21b0cc-645f-4a64-8b60-a0cc07692af8)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3-8B.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3-8B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.config.update(config)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"stevhliu/my_awesome_model\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
