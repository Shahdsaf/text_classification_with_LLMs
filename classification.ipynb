{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#environment setup\n",
    "#use RTL 4090 for finetuning models and A100-80GB for ICL with LLaMA 3-70B models\n",
    "!pip install transformers datasets accelerate scikit-learn evaluate wandb bitsandbytes peft --quiet\n",
    "!FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE pip install flash-attn --no-build-isolation --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data curation and splitting \n",
    "please skip this part if you want to use the split I did which is in the parquet files under data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read raw data\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "with open(\"data.json\", \"r\") as f:\n",
    "    data = json.load(f)[\"data\"]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 2 identical categories: \"Off-topic\" and \"Off-Topic\"\n",
    "# rename to \"Off-Topic\"\n",
    "\n",
    "df[\"category\"] = df[\"category\"].str.replace(\"Off-topic\", \"Off-Topic\")\n",
    "\n",
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given labels are not equally distributed, split by category with stratify\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train, test = train_test_split(\n",
    "    df, test_size=0.15, stratify=df[\"category\"], random_state=42, shuffle=True\n",
    ")\n",
    "train, val = train_test_split(\n",
    "    train, test_size=0.15, stratify=train[\"category\"], random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index and save to parquet\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "# for each row, add to question column f\"Question: {row['question']}\" and answer column f\"Answer: {row['answer']}\" and create a new column combined question and answer in one column\n",
    "\n",
    "train[\"question\"] = train[\"question\"].apply(lambda x: f\"Question: {x}\")\n",
    "train[\"answer\"] = train[\"answer\"].apply(lambda x: f\"Answer: {x}\")\n",
    "\n",
    "val[\"question\"] = val[\"question\"].apply(lambda x: f\"Question: {x}\")\n",
    "val[\"answer\"] = val[\"answer\"].apply(lambda x: f\"Answer: {x}\")\n",
    "\n",
    "test[\"question\"] = test[\"question\"].apply(lambda x: f\"Question: {x}\")\n",
    "test[\"answer\"] = test[\"answer\"].apply(lambda x: f\"Answer: {x}\")\n",
    "\n",
    "train[\"combined\"] = train[\"question\"] + \"\\n\" + train[\"answer\"]\n",
    "val[\"combined\"] = val[\"question\"] + \"\\n\" + val[\"answer\"]\n",
    "test[\"combined\"] = test[\"question\"] + \"\\n\" + test[\"answer\"]\n",
    "\n",
    "\n",
    "train.to_parquet(\"train.parquet\")\n",
    "val.to_parquet(\"val.parquet\")\n",
    "test.to_parquet(\"test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and tokenization for finetuning or ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and val from parquet\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train = pd.read_parquet(\"train.parquet\")\n",
    "val = pd.read_parquet(\"val.parquet\")\n",
    "test = pd.read_parquet(\"test.parquet\")\n",
    "\n",
    "# change column name from category to label which is used in HF datasets\n",
    "train = train.rename(columns={\"category\": \"label\"})\n",
    "val = val.rename(columns={\"category\": \"label\"})\n",
    "test = test.rename(columns={\"category\": \"label\"})\n",
    "\n",
    "train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nan values for train and val and test\n",
    "\n",
    "print(train.isna().sum())\n",
    "print(val.isna().sum())\n",
    "print(test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casting to class labels and loading to HF datasets\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train, split=\"train\").class_encode_column(\"label\")\n",
    "class_label_feature = train_dataset.features[\"label\"]\n",
    "val_dataset = Dataset.from_pandas(val, split=\"val\").cast_column(\n",
    "    \"label\", class_label_feature\n",
    ")\n",
    "test_dataset = Dataset.from_pandas(test, split=\"test\").cast_column(\n",
    "    \"label\", class_label_feature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label2id and id2label for the model\n",
    "label2id = train_dataset.features[\"label\"]._str2int\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# label2id = train_dataset.features['label'].names\n",
    "# id2label = {i: label for i, label in enumerate(label2id)}\n",
    "\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used only for finetuning with weighted crossentropy\n",
    "\n",
    "import torch\n",
    "\n",
    "label_id_df = train[\"label\"].map(label2id)\n",
    "\n",
    "class_weights = (1 / label_id_df.value_counts(normalize=True).sort_index()).tolist()\n",
    "class_weights = torch.tensor(class_weights)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model and training config and wandb config\n",
    "\n",
    "%env WANDB_WATCH=all\n",
    "%env WANDB_SILENT=true\n",
    "#%env WANDB_LOG_MODEL=end\n",
    "%env WANDB_LOG_MODEL=false\n",
    "%env WANDB_PROJECT=kapa_question_type_classification\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"input_type\": \"question\",   # question or combined which refers to question + answer\n",
    "    \"version\": \"1\",    #run number\n",
    "    \"batch_size\": 16,\n",
    "    \"train_epochs\": 100,\n",
    "    \"num_workers\": 8,\n",
    "    \"lr\": 1e-4,\n",
    "    #\"dropout\": 0.3\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer and preprocessing\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "if \"llama\" in config[\"model_name\"]:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        token=\"HF_token\",\n",
    "        add_prefix_space=True,\n",
    "    )\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"llama tokenizer\")\n",
    "\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        token=\"HF_token\",\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[config[\"input_type\"]], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data collator with dynamic padding\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics: f1, precision, recall (weighted mode) and f1 per label\n",
    "\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.combine([\"f1\", \"precision\", \"recall\"])\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    metrics = metric.compute(\n",
    "        predictions=predictions, references=labels, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    # F1 Score per label\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    f1_per_label = f1_metric.compute(\n",
    "        predictions=predictions, references=labels, average=None\n",
    "    )\n",
    "    f1_per_label_dict = {id2label[idx]: v for idx, v in enumerate(f1_per_label[\"f1\"])}\n",
    "\n",
    "    return {\n",
    "        \"f1\": metrics[\"f1\"],\n",
    "        \"precision\": metrics[\"precision\"],\n",
    "        \"recall\": metrics[\"recall\"],\n",
    "        \"f1_per_label\": f1_per_label_dict,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "if \"llama\" in config[\"model_name\"]:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        num_labels=6,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        token=\"HF_token\",\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    print(\"llama model\")\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config[\"model_name\"],\n",
    "        num_labels=6,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        token=\"HF_token\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the lora config and use this cell only if LLAMA 3-8B is used\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell only if dropout to be changed for Bert-like models\n",
    "# import torch.nn as nn\n",
    "\n",
    "# model.dropout = nn.Dropout(config[\"dropout\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params\n",
    "\n",
    "\n",
    "count_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training arguments\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=4)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{config['model_name']}-v-{config['version']}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=config[\"lr\"],\n",
    "    per_device_train_batch_size=config[\"batch_size\"],\n",
    "    per_device_eval_batch_size=config[\"batch_size\"],\n",
    "    num_train_epochs=config[\"train_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    push_to_hub=False,\n",
    "    # fp16=True,\n",
    "    # fp16_full_eval=True,\n",
    "    bf16_full_eval=True,\n",
    "    bf16=True,\n",
    "    save_total_limit=1,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"wandb\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    run_name=f\"{config['model_name']}-v-{config['version']}\",\n",
    "    dataloader_num_workers=config[\"num_workers\"],\n",
    "    # gradient_accumulation_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the trainer that performs the training with weighted crossentropy\n",
    "# use only for finetuning with weighted crossentropy\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(\n",
    "                self.args.device\n",
    "            )\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"label\").long()\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "# trainer = CustomTrainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=tokenized_train,\n",
    "#     eval_dataset=tokenized_val,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     callbacks=[early_stopping],\n",
    "#     data_collator=data_collator,\n",
    "#     class_weights=class_weights,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model and finish the wandb run\n",
    "import wandb\n",
    "\n",
    "wandb.config.update(config)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero/Few Shot In-Context Learning (ICL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the pipeline with LLaMA 3-70B\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_storage=torch.float16,\n",
    ")\n",
    "\n",
    "# model_id = \"meta-llama/Meta-Llama-3-70B\"\n",
    "model_id = (\n",
    "    \"meta-llama/Meta-Llama-3.1-70B-Instruct\"  # \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=\"HF_token\",  # add_prefix_space=True\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\n",
    "        \"quantization_config\": bnb_config,\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "        \"attn_implementation\": \"sdpa\",\n",
    "    },\n",
    "    device_map=\"auto\",\n",
    "    token=\"HF_token\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define few-shot examples\n",
    "# use in case of few shot ICL with random selection of few-shot examples\n",
    "nr_of_shots = 1\n",
    "sampled_questions = (\n",
    "    train.groupby(\"label\")\n",
    "    .apply(lambda x: x.sample(n=nr_of_shots))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create a defaultdict with list as the default factory\n",
    "label_question_dict = defaultdict(list)\n",
    "\n",
    "# Iterate over the DataFrame rows and append questions to the appropriate label list\n",
    "for _, row in sampled_questions.iterrows():\n",
    "    label_question_dict[row[\"label\"]].append(row[\"question\"])\n",
    "\n",
    "# Convert defaultdict to a regular dictionary if desired\n",
    "label_question_dict = dict(label_question_dict)\n",
    "\n",
    "one_shot_examples = label_question_dict\n",
    "\n",
    "one_shot_examples_string = \"\"\n",
    "for label, questions in one_shot_examples.items():\n",
    "    for question in questions:\n",
    "        question = question.replace(\"Question: \", \"\")\n",
    "        one_shot_examples_string += f\"Question: {question}. Answer: {label}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that finds the top N best matching questions per label for a given query question and\n",
    "# format these as a string to be used in the instruct prompt\n",
    "# use in case of few shot ICL with a heuristic selection of few-shot examples\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "val_embeddings = np.load(\"val_question_bert_embeddings.npy\")\n",
    "train_embeddings = np.load(\"train_question_bert_embeddings.npy\")\n",
    "\n",
    "\n",
    "def find_top_matches_per_label(val_embeddings, train_embeddings, train_df, top_n=2):\n",
    "    \"\"\"\n",
    "    Finds the top N best matching questions per label in the training set for each query in the validation set.\n",
    "\n",
    "    Parameters:\n",
    "    - val_embeddings (numpy.ndarray): The embeddings of the validation set questions.\n",
    "    - train_embeddings (numpy.ndarray): The embeddings of the training set questions.\n",
    "    - train_df (pandas.DataFrame): The training DataFrame containing \"question\" and \"label\" columns.\n",
    "    - top_n (int): The number of top matches per label to return for each validation query.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where each key is a validation query index and the value is a dictionary\n",
    "            with labels as keys and the top N matching questions as values.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over each validation query\n",
    "    for i, query_embedding in enumerate(val_embeddings):\n",
    "        # Compute similarities between the query and all training embeddings\n",
    "        similarities = np.dot(train_embeddings, query_embedding)\n",
    "\n",
    "        # Create a DataFrame to store similarity scores with corresponding questions and labels\n",
    "        similarity_df = pd.DataFrame(\n",
    "            {\n",
    "                \"question\": train_df[\"question\"],\n",
    "                \"label\": train_df[\"label\"],\n",
    "                \"similarity\": similarities,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Group by label and get the top N matches per label\n",
    "        top_matches_per_label = (\n",
    "            similarity_df.groupby(\"label\")\n",
    "            .apply(lambda x: x.nlargest(top_n, \"similarity\"))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Store the results for this query\n",
    "        results[i] = {}\n",
    "        for label in top_matches_per_label[\"label\"].unique():\n",
    "            best_matches = top_matches_per_label[\n",
    "                top_matches_per_label[\"label\"] == label\n",
    "            ][\"question\"].tolist()\n",
    "            results[i][label] = best_matches\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have your dataframes `val` and `train` and the corresponding embeddings `val_embeddings` and `train_embeddings`\n",
    "top_n = 1  # Number of top matches per label\n",
    "results = find_top_matches_per_label(val_embeddings, train_embeddings, train, top_n)\n",
    "\n",
    "\n",
    "one_shot_examples = []\n",
    "\n",
    "for i, one_shot_example in results.items():\n",
    "    one_shot_examples_string = \"\"\n",
    "    for label, questions in one_shot_example.items():\n",
    "        for question in questions:\n",
    "            question = question.replace(\"Question: \", \"\")\n",
    "            one_shot_examples_string += f\"Question: {question}. Answer: {label}\\n\\n\"\n",
    "\n",
    "    one_shot_examples.append(one_shot_examples_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform Zero-Shot or Few-Shot Classification\n",
    "# for zero shot remove from the prompt:\n",
    "#   {context}\n",
    "#   {task_info}\n",
    "#   {few_shots}\n",
    "# for few shot only use this cell if you rely on random selection of few-shot examples\n",
    "\n",
    "target = \"QUESTION TYPE\"\n",
    "target_with_brackets = f\"{{{target}}}\"\n",
    "labels = list(label2id.keys())\n",
    "label_token_lists = {}\n",
    "for l in list(label2id.keys()):\n",
    "    label_token_lists[l] = tokenizer.encode(l, add_special_tokens=False)\n",
    "\n",
    "\n",
    "def match_label(generated_string_tokens, ground_truth, label_token_lists):\n",
    "\n",
    "    def is_subsequence(subseq, seq):\n",
    "        \"\"\"\n",
    "        Check if subseq is a contiguous subsequence of seq.\n",
    "\n",
    "        Args:\n",
    "            subseq (list): The subsequence to check.\n",
    "            seq (list): The sequence to check against.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if subseq is a subsequence of seq, else False.\n",
    "        \"\"\"\n",
    "        sub_len = len(subseq)\n",
    "        for i in range(len(seq) - sub_len + 1):\n",
    "            if seq[i : i + sub_len] == subseq:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    matching_labels = []\n",
    "\n",
    "    # Check each label token list to see if it is a subsequence of the generated string tokens\n",
    "    for label, label_tokens in label_token_lists.items():\n",
    "        if is_subsequence(label_tokens, generated_string_tokens):\n",
    "            matching_labels.append(label)\n",
    "\n",
    "    # Return the label if only one matches, otherwise return None\n",
    "    if len(matching_labels) == 1 and matching_labels[0] == ground_truth:\n",
    "        return True  # matching_labels[0]\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "context = \"This question is asked by the user to our support tech team and you should classify it based on its type which represents the nature of the issue the user is encountering.\"\n",
    "\n",
    "task_info = \"\"\"This question asked by the user is of the following types:\n",
    "- `Discovery`: User is exploring how to do something or looking something up.\n",
    "- `Troubleshooting`: User is asking about an error, often this is a stacktrace.\n",
    "- `Code`: The user is inputing code and asking kapa to change, debug or explain something.\n",
    "- `Comparison`: User is asking kapa to contrast two things i.e. Are X and Y the same, what is the difference betwenn X and Y\n",
    "- `Advice`: The user is asking kapa what to do or what the best practice is.\n",
    "- `Off-Topic`: Anything else, could be something unrelated, gibberish, abuse and so on.\n",
    "\"\"\"\n",
    "\n",
    "few_shots = f\"\"\"Here are some examples of questions that are classified based on their type:\n",
    "\n",
    "{one_shot_examples_string}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def classify(question):\n",
    "    instructions = f\"\"\"\n",
    "    {context}\n",
    "    {task_info}\n",
    "    {few_shots}\n",
    "    \n",
    "    Return the {target} {labels} with one word/label response without any additional text. Answer: {target_with_brackets}.\"\"\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "    Question: {question}. Answer: \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "    ]\n",
    "    output = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=3,  # 64, 128, 256, 4096   #max_length\n",
    "        pad_token_id=pipeline.tokenizer.pad_token_id,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "        # top_k=1,\n",
    "        temperature=0.0,  # 0.1, 0.3, 0.6, 0.8\n",
    "        # top_p=0.9,  # 0.8, 0.9, 0.95\n",
    "        return_full_text=False,\n",
    "    )[0][\"generated_text\"].lstrip()\n",
    "    return output\n",
    "\n",
    "\n",
    "def final_output(question, ground_truth):\n",
    "    output = classify(question)\n",
    "    generated_string_tokens = tokenizer.encode(output, add_special_tokens=False)\n",
    "    if match_label(generated_string_tokens, ground_truth, label_token_lists):\n",
    "        return label2id[ground_truth]\n",
    "    else:\n",
    "        # choose random label from remaining ones excluding ground truth\n",
    "        remaining_labels = [l for l in labels if l != ground_truth]\n",
    "        random_label = np.random.choice(remaining_labels)\n",
    "        return label2id[random_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for few shot with a heuristic selection of few-shot examples\n",
    "\n",
    "\n",
    "target = \"QUESTION TYPE\"\n",
    "target_with_brackets = f\"{{{target}}}\"\n",
    "labels = list(label2id.keys())\n",
    "label_token_lists = {}\n",
    "for l in list(label2id.keys()):\n",
    "    label_token_lists[l] = tokenizer.encode(l, add_special_tokens=False)\n",
    "\n",
    "\n",
    "def match_label(generated_string_tokens, ground_truth, label_token_lists):\n",
    "\n",
    "    def is_subsequence(subseq, seq):\n",
    "        \"\"\"\n",
    "        Check if subseq is a contiguous subsequence of seq.\n",
    "\n",
    "        Args:\n",
    "            subseq (list): The subsequence to check.\n",
    "            seq (list): The sequence to check against.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if subseq is a subsequence of seq, else False.\n",
    "        \"\"\"\n",
    "        sub_len = len(subseq)\n",
    "        for i in range(len(seq) - sub_len + 1):\n",
    "            if seq[i : i + sub_len] == subseq:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    matching_labels = []\n",
    "\n",
    "    # Check each label token list to see if it is a subsequence of the generated string tokens\n",
    "    for label, label_tokens in label_token_lists.items():\n",
    "        if is_subsequence(label_tokens, generated_string_tokens):\n",
    "            matching_labels.append(label)\n",
    "\n",
    "    # Return the label if only one matches, otherwise return None\n",
    "    if len(matching_labels) == 1 and matching_labels[0] == ground_truth:\n",
    "        return True  # matching_labels[0]\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "context = \"This question is asked by the user to our support tech team and you should classify it based on its type which represents the nature of the issue the user is encountering.\"\n",
    "\n",
    "task_info = \"\"\"This question asked by the user is of the following types:\n",
    "- `Discovery`: User is exploring how to do something or looking something up.\n",
    "- `Troubleshooting`: User is asking about an error, often this is a stacktrace.\n",
    "- `Code`: The user is inputing code and asking kapa to change, debug or explain something.\n",
    "- `Comparison`: User is asking kapa to contrast two things i.e. Are X and Y the same, what is the difference betwenn X and Y\n",
    "- `Advice`: The user is asking kapa what to do or what the best practice is.\n",
    "- `Off-Topic`: Anything else, could be something unrelated, gibberish, abuse and so on.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def classify(question, one_shot_examples_string):\n",
    "\n",
    "    few_shots = f\"\"\"Here are some examples of questions that are classified based on their type:\n",
    "\n",
    "    {one_shot_examples_string}\n",
    "    \"\"\"\n",
    "    instructions = f\"\"\"\n",
    "    {context}\n",
    "    {task_info}\n",
    "    {few_shots}\n",
    "    \n",
    "    Return the {target} {labels} with one word/label response without any additional text. Answer: {target_with_brackets}.\"\"\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "    Question: {question}. Answer: \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "    ]\n",
    "    output = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=3,  # 64, 128, 256, 4096   #max_length\n",
    "        pad_token_id=pipeline.tokenizer.pad_token_id,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "        # top_k=1,\n",
    "        temperature=0.0,  # 0.1, 0.3, 0.6, 0.8\n",
    "        # top_p=0.9,  # 0.8, 0.9, 0.95\n",
    "        return_full_text=False,\n",
    "    )[0][\"generated_text\"].lstrip()\n",
    "    return output\n",
    "\n",
    "\n",
    "def final_output(question, ground_truth, one_shot_examples_string):\n",
    "    output = classify(question, one_shot_examples_string)\n",
    "    generated_string_tokens = tokenizer.encode(output, add_special_tokens=False)\n",
    "    if match_label(generated_string_tokens, ground_truth, label_token_lists):\n",
    "        return label2id[ground_truth]\n",
    "    else:\n",
    "        # choose random label from remaining ones excluding ground truth\n",
    "        remaining_labels = [l for l in labels if l != ground_truth]\n",
    "        random_label = np.random.choice(remaining_labels)\n",
    "        return label2id[random_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given val dataset, generate predictions\n",
    "\n",
    "questions = val[\"question\"].tolist()\n",
    "ground_truths = val[\"label\"].tolist()\n",
    "\n",
    "preds = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "for i in tqdm(range(len(questions))):\n",
    "    # one_shot_examples_string = one_shot_examples[i]\n",
    "    question = questions[i]\n",
    "    ground_truth = ground_truths[i]\n",
    "    # pred = final_output(question, ground_truth, one_shot_examples_string)\n",
    "    pred = final_output(question, ground_truth)\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ground truths to ids\n",
    "ground_truths_ids = [label2id[l] for l in ground_truths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given preds and ground truth, calculate f1 score weighted\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.combine([\"f1\", \"precision\", \"recall\"])\n",
    "metrics = metric.compute(\n",
    "    predictions=preds, references=ground_truths_ids, average=\"weighted\"\n",
    ")\n",
    "# F1 Score per label\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "f1_per_label = f1_metric.compute(\n",
    "    predictions=preds, references=ground_truths_ids, average=None\n",
    ")\n",
    "f1_per_label_dict = {id2label[idx]: v for idx, v in enumerate(f1_per_label[\"f1\"])}\n",
    "\n",
    "metrics.update(f1_per_label_dict)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Bert embeddings for train or val data and save them to disk\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Check if CUDA is available and use it if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embeddings(text):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "        )\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        # Get the embeddings of [CLS] token, which is the first token\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Initialize an empty list to store the embeddings\n",
    "embeddings_list = []\n",
    "\n",
    "# Iterate over the questions in the dataset with a progress bar\n",
    "for question in tqdm(val_dataset[\"question\"], desc=\"Generating embeddings\"):\n",
    "    embedding = get_embeddings(question)\n",
    "    embedding = embedding / norm(embedding, axis=1, keepdims=True)\n",
    "    embeddings_list.append(embedding)\n",
    "\n",
    "# Convert the list of embeddings to a NumPy array\n",
    "embeddings = np.vstack(embeddings_list)\n",
    "\n",
    "# Save embeddings to a numpy file\n",
    "np.save(\"val_question_bert_embeddings.npy\", embeddings)\n",
    "\n",
    "print(\"Embeddings saved to 'question_embeddings.npy'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
